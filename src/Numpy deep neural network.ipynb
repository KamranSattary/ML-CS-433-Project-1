{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-55-d0319d5f013e>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "<ipython-input-55-d0319d5f013e>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    }
   ],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-60-f0057354f996>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "<ipython-input-60-f0057354f996>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    }
   ],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # step backward - calculating gradient\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        # updating model state\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if verbose:\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if callback is not None :\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data\n",
    "from implementations import lasso_reg\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "GAMMA = 0.01\n",
    "MAX_ITER = 100\n",
    "LAMBDA = 1e-9\n",
    "\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# init_weights = np.random.random_sample((tX.shape[1], 1))\n",
    "# w, _ = lasso_reg(y.reshape((-1, 1)), tX, init_weights, 40, GAMMA, LAMBDA)\n",
    "#\n",
    "# removed_features = np.where(w == 0)\n",
    "# tX = np.delete(tX, removed_features, axis=1)\n",
    "# np.isnan(tX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.124060e+02  4.652400e+01  7.375200e+01  3.846750e+01  2.107000e+00\n",
      "  2.258850e+02 -2.440000e-01  2.491500e+00  1.231550e+01  1.206645e+02\n",
      "  1.280000e+00 -3.560000e-01  4.540000e-01  3.180400e+01 -2.300000e-02\n",
      " -3.300000e-02  4.051600e+01 -4.500000e-02  8.600000e-02  3.480200e+01\n",
      " -2.400000e-02  1.797390e+02  1.000000e+00  6.556100e+01  0.000000e+00\n",
      " -3.300000e-02  4.790200e+01 -1.000000e-02 -2.000000e-03  4.051250e+01]\n",
      "[ 1.20417434e+02  4.92398193e+01  8.11819816e+01  5.78959617e+01\n",
      "  2.19310420e+00  2.68220619e+02 -4.11628932e-01  2.37309984e+00\n",
      "  1.89173324e+01  1.58432217e+02  1.43760943e+00 -1.28304708e-01\n",
      "  4.55244780e-01  3.87074191e+01 -1.09730480e-02 -8.17107200e-03\n",
      "  4.66602072e+01 -1.95074680e-02  4.35429640e-02  4.17172345e+01\n",
      " -1.01191920e-02  2.09797178e+02  9.79176000e-01  7.71243656e+01\n",
      " -1.96589200e-03 -2.06285240e-02  5.07391493e+01 -1.05354440e-02\n",
      " -1.87879200e-03  7.30645914e+01]\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(tX == -999)\n",
    "tX[inds] = np.nan\n",
    "\n",
    "col_mean = np.nanmedian(tX, axis=0)\n",
    "print(col_mean)\n",
    "\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(tX))\n",
    "\n",
    "#Place column means in the indices. Align the arrays using ta`ke\n",
    "tX[inds] = np.take(col_mean, inds[1])\n",
    "print(np.nanmean(tX, axis=0))\n",
    "\n",
    "xmin, xmax = np.min(tX, axis=0), np.max(tX, axis=0)\n",
    "tX = (tX - xmin) / (xmax-xmin)\n",
    "\n",
    "print(tX.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_weights = np.random.random_sample((tX.shape[1], 1))\n",
    "w, _ = lasso_reg(y.reshape((-1, 1)), tX, init_weights, 100, GAMMA, LAMBDA)\n",
    "\n",
    "removed_features = np.where(w == 0)\n",
    "tX = np.delete(tX, removed_features, axis=1)\n",
    "\n",
    "print(\"remaining features {}\", len(tX[0]))\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": len(tX[0]), \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 8, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 8, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 0.68314 - accuracy: 0.65733\n",
      "Iteration: 00010 - cost: 0.64877 - accuracy: 0.65733\n",
      "Iteration: 00020 - cost: 0.64307 - accuracy: 0.65733\n",
      "Iteration: 00030 - cost: 0.64197 - accuracy: 0.65733\n",
      "Iteration: 00040 - cost: 0.64154 - accuracy: 0.65733\n",
      "Iteration: 00050 - cost: 0.64121 - accuracy: 0.65733\n",
      "Iteration: 00060 - cost: 0.64089 - accuracy: 0.65733\n",
      "Iteration: 00070 - cost: 0.64054 - accuracy: 0.65733\n",
      "Iteration: 00080 - cost: 0.64016 - accuracy: 0.65733\n",
      "Iteration: 00090 - cost: 0.63972 - accuracy: 0.65733\n",
      "Iteration: 00100 - cost: 0.63922 - accuracy: 0.65733\n",
      "Iteration: 00110 - cost: 0.63864 - accuracy: 0.65733\n",
      "Iteration: 00120 - cost: 0.63798 - accuracy: 0.65733\n",
      "Iteration: 00130 - cost: 0.63725 - accuracy: 0.65733\n",
      "Iteration: 00140 - cost: 0.63643 - accuracy: 0.65733\n",
      "Iteration: 00150 - cost: 0.63551 - accuracy: 0.65733\n",
      "Iteration: 00160 - cost: 0.63446 - accuracy: 0.65733\n",
      "Iteration: 00170 - cost: 0.63327 - accuracy: 0.65733\n",
      "Iteration: 00180 - cost: 0.63190 - accuracy: 0.65733\n",
      "Iteration: 00190 - cost: 0.63030 - accuracy: 0.65733\n",
      "Iteration: 00200 - cost: 0.62838 - accuracy: 0.65733\n",
      "Iteration: 00210 - cost: 0.62609 - accuracy: 0.65733\n",
      "Iteration: 00220 - cost: 0.62367 - accuracy: 0.65733\n",
      "Iteration: 00230 - cost: 0.62110 - accuracy: 0.65733\n",
      "Iteration: 00240 - cost: 0.61835 - accuracy: 0.65733\n",
      "Iteration: 00250 - cost: 0.61545 - accuracy: 0.65733\n",
      "Iteration: 00260 - cost: 0.61247 - accuracy: 0.65733\n",
      "Iteration: 00270 - cost: 0.60951 - accuracy: 0.65733\n",
      "Iteration: 00280 - cost: 0.60669 - accuracy: 0.65778\n",
      "Iteration: 00290 - cost: 0.60411 - accuracy: 0.66568\n",
      "Iteration: 00300 - cost: 0.60181 - accuracy: 0.67404\n",
      "Iteration: 00310 - cost: 0.59979 - accuracy: 0.68042\n",
      "Iteration: 00320 - cost: 0.59801 - accuracy: 0.68566\n",
      "Iteration: 00330 - cost: 0.59646 - accuracy: 0.68916\n",
      "Iteration: 00340 - cost: 0.59506 - accuracy: 0.69120\n",
      "Iteration: 00350 - cost: 0.59380 - accuracy: 0.69250\n",
      "Iteration: 00360 - cost: 0.59262 - accuracy: 0.69361\n",
      "Iteration: 00370 - cost: 0.59150 - accuracy: 0.69400\n",
      "Iteration: 00380 - cost: 0.59041 - accuracy: 0.69462\n",
      "Iteration: 00390 - cost: 0.58932 - accuracy: 0.69526\n",
      "Iteration: 00400 - cost: 0.58822 - accuracy: 0.69630\n",
      "Iteration: 00410 - cost: 0.58715 - accuracy: 0.69722\n",
      "Iteration: 00420 - cost: 0.58613 - accuracy: 0.69822\n",
      "Iteration: 00430 - cost: 0.58512 - accuracy: 0.69929\n",
      "Iteration: 00440 - cost: 0.58412 - accuracy: 0.70010\n",
      "Iteration: 00450 - cost: 0.58314 - accuracy: 0.70096\n",
      "Iteration: 00460 - cost: 0.58218 - accuracy: 0.70178\n",
      "Iteration: 00470 - cost: 0.58122 - accuracy: 0.70276\n",
      "Iteration: 00480 - cost: 0.58025 - accuracy: 0.70358\n",
      "Iteration: 00490 - cost: 0.57928 - accuracy: 0.70442\n",
      "Iteration: 00500 - cost: 0.57831 - accuracy: 0.70502\n",
      "Iteration: 00510 - cost: 0.57731 - accuracy: 0.70594\n",
      "Iteration: 00520 - cost: 0.57617 - accuracy: 0.70638\n",
      "Iteration: 00530 - cost: 0.57488 - accuracy: 0.70705\n",
      "Iteration: 00540 - cost: 0.57367 - accuracy: 0.70800\n",
      "Iteration: 00550 - cost: 0.57250 - accuracy: 0.70871\n",
      "Iteration: 00560 - cost: 0.57133 - accuracy: 0.70934\n",
      "Iteration: 00570 - cost: 0.57013 - accuracy: 0.71009\n",
      "Iteration: 00580 - cost: 0.56891 - accuracy: 0.71072\n",
      "Iteration: 00590 - cost: 0.56766 - accuracy: 0.71127\n",
      "Iteration: 00600 - cost: 0.56636 - accuracy: 0.71184\n",
      "Iteration: 00610 - cost: 0.56502 - accuracy: 0.71248\n",
      "Iteration: 00620 - cost: 0.56363 - accuracy: 0.71332\n",
      "Iteration: 00630 - cost: 0.56219 - accuracy: 0.71410\n",
      "Iteration: 00640 - cost: 0.56068 - accuracy: 0.71476\n",
      "Iteration: 00650 - cost: 0.55912 - accuracy: 0.71558\n",
      "Iteration: 00660 - cost: 0.55748 - accuracy: 0.71622\n",
      "Iteration: 00670 - cost: 0.56968 - accuracy: 0.69822\n",
      "Iteration: 00680 - cost: 0.59303 - accuracy: 0.69258\n",
      "Iteration: 00690 - cost: 0.56851 - accuracy: 0.69872\n",
      "Iteration: 00700 - cost: 0.56832 - accuracy: 0.69870\n",
      "Iteration: 00710 - cost: 0.57396 - accuracy: 0.69734\n",
      "Iteration: 00720 - cost: 0.56927 - accuracy: 0.69829\n",
      "Iteration: 00730 - cost: 0.56815 - accuracy: 0.69845\n",
      "Iteration: 00740 - cost: 0.56991 - accuracy: 0.69816\n",
      "Iteration: 00750 - cost: 0.56763 - accuracy: 0.69863\n",
      "Iteration: 00760 - cost: 0.56766 - accuracy: 0.69872\n",
      "Iteration: 00770 - cost: 0.56779 - accuracy: 0.69846\n",
      "Iteration: 00780 - cost: 0.56656 - accuracy: 0.69884\n",
      "Iteration: 00790 - cost: 0.56669 - accuracy: 0.69874\n",
      "Iteration: 00800 - cost: 0.56612 - accuracy: 0.69884\n",
      "Iteration: 00810 - cost: 0.56573 - accuracy: 0.69883\n",
      "Iteration: 00820 - cost: 0.56547 - accuracy: 0.69896\n",
      "Iteration: 00830 - cost: 0.56489 - accuracy: 0.69909\n",
      "Iteration: 00840 - cost: 0.56457 - accuracy: 0.69907\n",
      "Iteration: 00850 - cost: 0.56414 - accuracy: 0.69917\n",
      "Iteration: 00860 - cost: 0.56359 - accuracy: 0.69931\n",
      "Iteration: 00870 - cost: 0.56318 - accuracy: 0.69938\n",
      "Iteration: 00880 - cost: 0.56272 - accuracy: 0.69944\n",
      "Iteration: 00890 - cost: 0.56236 - accuracy: 0.69956\n",
      "Iteration: 00900 - cost: 0.56185 - accuracy: 0.69972\n",
      "Iteration: 00910 - cost: 0.56135 - accuracy: 0.69978\n",
      "Iteration: 00920 - cost: 0.56088 - accuracy: 0.69993\n",
      "Iteration: 00930 - cost: 0.56041 - accuracy: 0.70006\n",
      "Iteration: 00940 - cost: 0.55992 - accuracy: 0.70023\n",
      "Iteration: 00950 - cost: 0.55949 - accuracy: 0.70033\n",
      "Iteration: 00960 - cost: 0.55888 - accuracy: 0.70046\n",
      "Iteration: 00970 - cost: 0.55837 - accuracy: 0.70061\n",
      "Iteration: 00980 - cost: 0.55784 - accuracy: 0.70076\n",
      "Iteration: 00990 - cost: 0.55734 - accuracy: 0.70095\n",
      "Iteration: 01000 - cost: 0.55678 - accuracy: 0.70116\n",
      "Iteration: 01010 - cost: 0.55617 - accuracy: 0.70132\n",
      "Iteration: 01020 - cost: 0.55557 - accuracy: 0.70152\n",
      "Iteration: 01030 - cost: 0.55504 - accuracy: 0.70167\n",
      "Iteration: 01040 - cost: 0.55455 - accuracy: 0.70176\n",
      "Iteration: 01050 - cost: 0.55388 - accuracy: 0.70191\n",
      "Iteration: 01060 - cost: 0.55342 - accuracy: 0.70214\n",
      "Iteration: 01070 - cost: 0.55296 - accuracy: 0.70230\n",
      "Iteration: 01080 - cost: 0.55228 - accuracy: 0.70257\n",
      "Iteration: 01090 - cost: 0.55191 - accuracy: 0.70276\n",
      "Iteration: 01100 - cost: 0.55131 - accuracy: 0.70299\n",
      "Iteration: 01110 - cost: 0.55100 - accuracy: 0.70299\n",
      "Iteration: 01120 - cost: 0.55034 - accuracy: 0.70330\n",
      "Iteration: 01130 - cost: 0.54976 - accuracy: 0.70347\n",
      "Iteration: 01140 - cost: 0.54933 - accuracy: 0.70351\n",
      "Iteration: 01150 - cost: 0.54874 - accuracy: 0.70374\n",
      "Iteration: 01160 - cost: 0.54834 - accuracy: 0.70388\n",
      "Iteration: 01170 - cost: 0.54780 - accuracy: 0.70400\n",
      "Iteration: 01180 - cost: 0.54727 - accuracy: 0.70412\n",
      "Iteration: 01190 - cost: 0.54668 - accuracy: 0.70433\n",
      "Iteration: 01200 - cost: 0.54627 - accuracy: 0.70430\n",
      "Iteration: 01210 - cost: 0.54564 - accuracy: 0.70450\n",
      "Iteration: 01220 - cost: 0.54501 - accuracy: 0.70465\n",
      "Iteration: 01230 - cost: 0.54457 - accuracy: 0.70474\n",
      "Iteration: 01240 - cost: 0.54399 - accuracy: 0.70471\n",
      "Iteration: 01250 - cost: 0.54335 - accuracy: 0.70482\n",
      "Iteration: 01260 - cost: 0.54284 - accuracy: 0.70484\n",
      "Iteration: 01270 - cost: 0.54213 - accuracy: 0.70496\n",
      "Iteration: 01280 - cost: 0.54166 - accuracy: 0.70504\n",
      "Iteration: 01290 - cost: 0.54112 - accuracy: 0.70515\n",
      "Iteration: 01300 - cost: 0.54073 - accuracy: 0.70503\n",
      "Iteration: 01310 - cost: 0.54035 - accuracy: 0.70518\n",
      "Iteration: 01320 - cost: 0.53978 - accuracy: 0.70524\n",
      "Iteration: 01330 - cost: 0.53949 - accuracy: 0.70535\n",
      "Iteration: 01340 - cost: 0.53896 - accuracy: 0.70542\n",
      "Iteration: 01350 - cost: 0.53858 - accuracy: 0.70548\n",
      "Iteration: 01360 - cost: 0.53817 - accuracy: 0.70560\n",
      "Iteration: 01370 - cost: 0.53778 - accuracy: 0.70573\n",
      "Iteration: 01380 - cost: 0.53737 - accuracy: 0.70597\n",
      "Iteration: 01390 - cost: 0.53691 - accuracy: 0.70618\n",
      "Iteration: 01400 - cost: 0.53635 - accuracy: 0.70633\n",
      "Iteration: 01410 - cost: 0.53587 - accuracy: 0.70656\n",
      "Iteration: 01420 - cost: 0.53539 - accuracy: 0.70680\n",
      "Iteration: 01430 - cost: 0.53495 - accuracy: 0.70698\n",
      "Iteration: 01440 - cost: 0.53442 - accuracy: 0.70726\n",
      "Iteration: 01450 - cost: 0.53384 - accuracy: 0.70761\n",
      "Iteration: 01460 - cost: 0.53336 - accuracy: 0.70783\n",
      "Iteration: 01470 - cost: 0.53281 - accuracy: 0.70810\n",
      "Iteration: 01480 - cost: 0.53241 - accuracy: 0.70823\n",
      "Iteration: 01490 - cost: 0.53190 - accuracy: 0.70850\n",
      "Iteration: 01500 - cost: 0.53141 - accuracy: 0.70876\n",
      "Iteration: 01510 - cost: 0.53095 - accuracy: 0.70905\n",
      "Iteration: 01520 - cost: 0.53055 - accuracy: 0.70923\n",
      "Iteration: 01530 - cost: 0.53001 - accuracy: 0.70966\n",
      "Iteration: 01540 - cost: 0.52953 - accuracy: 0.70997\n",
      "Iteration: 01550 - cost: 0.52911 - accuracy: 0.71020\n",
      "Iteration: 01560 - cost: 0.52869 - accuracy: 0.71044\n",
      "Iteration: 01570 - cost: 0.52809 - accuracy: 0.71091\n",
      "Iteration: 01580 - cost: 0.52767 - accuracy: 0.71114\n",
      "Iteration: 01590 - cost: 0.52727 - accuracy: 0.71144\n",
      "Iteration: 01600 - cost: 0.52674 - accuracy: 0.71175\n",
      "Iteration: 01610 - cost: 0.52631 - accuracy: 0.71208\n",
      "Iteration: 01620 - cost: 0.52585 - accuracy: 0.71242\n",
      "Iteration: 01630 - cost: 0.52546 - accuracy: 0.71264\n",
      "Iteration: 01640 - cost: 0.52489 - accuracy: 0.71318\n",
      "Iteration: 01650 - cost: 0.52449 - accuracy: 0.71354\n",
      "Iteration: 01660 - cost: 0.52401 - accuracy: 0.71396\n",
      "Iteration: 01670 - cost: 0.52356 - accuracy: 0.71437\n",
      "Iteration: 01680 - cost: 0.52320 - accuracy: 0.71473\n",
      "Iteration: 01690 - cost: 0.52270 - accuracy: 0.71520\n",
      "Iteration: 01700 - cost: 0.52230 - accuracy: 0.71544\n",
      "Iteration: 01710 - cost: 0.52182 - accuracy: 0.71583\n",
      "Iteration: 01720 - cost: 0.52145 - accuracy: 0.71614\n",
      "Iteration: 01730 - cost: 0.52103 - accuracy: 0.71654\n",
      "Iteration: 01740 - cost: 0.52064 - accuracy: 0.71694\n",
      "Iteration: 01750 - cost: 0.52018 - accuracy: 0.71727\n",
      "Iteration: 01760 - cost: 0.51982 - accuracy: 0.71750\n",
      "Iteration: 01770 - cost: 0.51944 - accuracy: 0.71784\n",
      "Iteration: 01780 - cost: 0.51895 - accuracy: 0.71820\n",
      "Iteration: 01790 - cost: 0.51842 - accuracy: 0.71864\n",
      "Iteration: 01800 - cost: 0.51811 - accuracy: 0.71890\n",
      "Iteration: 01810 - cost: 0.51763 - accuracy: 0.71930\n",
      "Iteration: 01820 - cost: 0.51723 - accuracy: 0.71981\n",
      "Iteration: 01830 - cost: 0.51684 - accuracy: 0.72022\n",
      "Iteration: 01840 - cost: 0.51645 - accuracy: 0.72051\n",
      "Iteration: 01850 - cost: 0.51605 - accuracy: 0.72091\n",
      "Iteration: 01860 - cost: 0.51567 - accuracy: 0.72114\n",
      "Iteration: 01870 - cost: 0.51532 - accuracy: 0.72150\n",
      "Iteration: 01880 - cost: 0.51494 - accuracy: 0.72177\n",
      "Iteration: 01890 - cost: 0.51456 - accuracy: 0.72217\n",
      "Iteration: 01900 - cost: 0.51406 - accuracy: 0.72265\n",
      "Iteration: 01910 - cost: 0.51367 - accuracy: 0.72295\n",
      "Iteration: 01920 - cost: 0.51326 - accuracy: 0.72333\n",
      "Iteration: 01930 - cost: 0.51293 - accuracy: 0.72369\n",
      "Iteration: 01940 - cost: 0.51245 - accuracy: 0.72412\n",
      "Iteration: 01950 - cost: 0.51205 - accuracy: 0.72444\n",
      "Iteration: 01960 - cost: 0.51173 - accuracy: 0.72479\n",
      "Iteration: 01970 - cost: 0.51135 - accuracy: 0.72506\n",
      "Iteration: 01980 - cost: 0.51100 - accuracy: 0.72522\n",
      "Iteration: 01990 - cost: 0.51060 - accuracy: 0.72557\n",
      "Iteration: 02000 - cost: 0.51029 - accuracy: 0.72584\n",
      "Iteration: 02010 - cost: 0.50994 - accuracy: 0.72614\n",
      "Iteration: 02020 - cost: 0.50951 - accuracy: 0.72649\n",
      "Iteration: 02030 - cost: 0.50910 - accuracy: 0.72683\n",
      "Iteration: 02040 - cost: 0.50876 - accuracy: 0.72712\n",
      "Iteration: 02050 - cost: 0.50843 - accuracy: 0.72751\n",
      "Iteration: 02060 - cost: 0.50809 - accuracy: 0.72779\n",
      "Iteration: 02070 - cost: 0.50776 - accuracy: 0.72804\n",
      "Iteration: 02080 - cost: 0.50743 - accuracy: 0.72835\n",
      "Iteration: 02090 - cost: 0.50710 - accuracy: 0.72870\n",
      "Iteration: 02100 - cost: 0.50676 - accuracy: 0.72902\n",
      "Iteration: 02110 - cost: 0.50634 - accuracy: 0.72951\n",
      "Iteration: 02120 - cost: 0.50605 - accuracy: 0.72970\n",
      "Iteration: 02130 - cost: 0.50576 - accuracy: 0.72994\n",
      "Iteration: 02140 - cost: 0.50544 - accuracy: 0.73022\n",
      "Iteration: 02150 - cost: 0.50508 - accuracy: 0.73055\n",
      "Iteration: 02160 - cost: 0.50473 - accuracy: 0.73093\n",
      "Iteration: 02170 - cost: 0.50441 - accuracy: 0.73126\n",
      "Iteration: 02180 - cost: 0.50403 - accuracy: 0.73164\n",
      "Iteration: 02190 - cost: 0.50367 - accuracy: 0.73199\n",
      "Iteration: 02200 - cost: 0.50340 - accuracy: 0.73218\n",
      "Iteration: 02210 - cost: 0.50312 - accuracy: 0.73238\n",
      "Iteration: 02220 - cost: 0.50276 - accuracy: 0.73265\n",
      "Iteration: 02230 - cost: 0.50243 - accuracy: 0.73305\n",
      "Iteration: 02240 - cost: 0.50219 - accuracy: 0.73326\n",
      "Iteration: 02250 - cost: 0.50191 - accuracy: 0.73360\n",
      "Iteration: 02260 - cost: 0.50154 - accuracy: 0.73386\n",
      "Iteration: 02270 - cost: 0.50123 - accuracy: 0.73419\n",
      "Iteration: 02280 - cost: 0.50093 - accuracy: 0.73442\n",
      "Iteration: 02290 - cost: 0.50062 - accuracy: 0.73472\n",
      "Iteration: 02300 - cost: 0.50030 - accuracy: 0.73496\n",
      "Iteration: 02310 - cost: 0.50006 - accuracy: 0.73518\n",
      "Iteration: 02320 - cost: 0.49971 - accuracy: 0.73550\n",
      "Iteration: 02330 - cost: 0.49942 - accuracy: 0.73576\n",
      "Iteration: 02340 - cost: 0.49908 - accuracy: 0.73606\n",
      "Iteration: 02350 - cost: 0.49882 - accuracy: 0.73626\n",
      "Iteration: 02360 - cost: 0.49851 - accuracy: 0.73655\n",
      "Iteration: 02370 - cost: 0.49818 - accuracy: 0.73689\n",
      "Iteration: 02380 - cost: 0.49791 - accuracy: 0.73718\n",
      "Iteration: 02390 - cost: 0.49760 - accuracy: 0.73746\n",
      "Iteration: 02400 - cost: 0.49727 - accuracy: 0.73775\n",
      "Iteration: 02410 - cost: 0.49699 - accuracy: 0.73799\n",
      "Iteration: 02420 - cost: 0.49673 - accuracy: 0.73823\n",
      "Iteration: 02430 - cost: 0.49644 - accuracy: 0.73856\n",
      "Iteration: 02440 - cost: 0.49615 - accuracy: 0.73887\n",
      "Iteration: 02450 - cost: 0.49582 - accuracy: 0.73916\n",
      "Iteration: 02460 - cost: 0.49550 - accuracy: 0.73951\n",
      "Iteration: 02470 - cost: 0.49522 - accuracy: 0.73965\n",
      "Iteration: 02480 - cost: 0.49493 - accuracy: 0.73997\n",
      "Iteration: 02490 - cost: 0.49469 - accuracy: 0.74020\n",
      "Iteration: 02500 - cost: 0.49436 - accuracy: 0.74053\n",
      "Iteration: 02510 - cost: 0.49407 - accuracy: 0.74084\n",
      "Iteration: 02520 - cost: 0.49381 - accuracy: 0.74118\n",
      "Iteration: 02530 - cost: 0.49350 - accuracy: 0.74153\n",
      "Iteration: 02540 - cost: 0.49321 - accuracy: 0.74180\n",
      "Iteration: 02550 - cost: 0.49290 - accuracy: 0.74211\n",
      "Iteration: 02560 - cost: 0.49259 - accuracy: 0.74240\n",
      "Iteration: 02570 - cost: 0.49232 - accuracy: 0.74262\n",
      "Iteration: 02580 - cost: 0.49203 - accuracy: 0.74302\n",
      "Iteration: 02590 - cost: 0.49171 - accuracy: 0.74337\n",
      "Iteration: 02600 - cost: 0.49146 - accuracy: 0.74362\n",
      "Iteration: 02610 - cost: 0.49120 - accuracy: 0.74386\n",
      "Iteration: 02620 - cost: 0.49092 - accuracy: 0.74413\n",
      "Iteration: 02630 - cost: 0.49066 - accuracy: 0.74437\n",
      "Iteration: 02640 - cost: 0.49037 - accuracy: 0.74478\n",
      "Iteration: 02650 - cost: 0.49018 - accuracy: 0.74500\n",
      "Iteration: 02660 - cost: 0.48987 - accuracy: 0.74537\n",
      "Iteration: 02670 - cost: 0.48960 - accuracy: 0.74568\n",
      "Iteration: 02680 - cost: 0.48938 - accuracy: 0.74592\n",
      "Iteration: 02690 - cost: 0.48911 - accuracy: 0.74627\n",
      "Iteration: 02700 - cost: 0.48886 - accuracy: 0.74660\n",
      "Iteration: 02710 - cost: 0.48857 - accuracy: 0.74685\n",
      "Iteration: 02720 - cost: 0.48831 - accuracy: 0.74718\n",
      "Iteration: 02730 - cost: 0.48812 - accuracy: 0.74732\n",
      "Iteration: 02740 - cost: 0.48787 - accuracy: 0.74771\n",
      "Iteration: 02750 - cost: 0.48760 - accuracy: 0.74799\n",
      "Iteration: 02760 - cost: 0.48733 - accuracy: 0.74822\n",
      "Iteration: 02770 - cost: 0.48710 - accuracy: 0.74848\n",
      "Iteration: 02780 - cost: 0.48684 - accuracy: 0.74881\n",
      "Iteration: 02790 - cost: 0.48661 - accuracy: 0.74902\n",
      "Iteration: 02800 - cost: 0.48631 - accuracy: 0.74921\n",
      "Iteration: 02810 - cost: 0.48608 - accuracy: 0.74934\n",
      "Iteration: 02820 - cost: 0.48584 - accuracy: 0.74965\n",
      "Iteration: 02830 - cost: 0.48563 - accuracy: 0.74992\n",
      "Iteration: 02840 - cost: 0.48540 - accuracy: 0.75010\n",
      "Iteration: 02850 - cost: 0.48517 - accuracy: 0.75027\n",
      "Iteration: 02860 - cost: 0.48489 - accuracy: 0.75058\n",
      "Iteration: 02870 - cost: 0.48472 - accuracy: 0.75079\n",
      "Iteration: 02880 - cost: 0.48450 - accuracy: 0.75104\n",
      "Iteration: 02890 - cost: 0.48424 - accuracy: 0.75126\n",
      "Iteration: 02900 - cost: 0.48399 - accuracy: 0.75140\n",
      "Iteration: 02910 - cost: 0.48372 - accuracy: 0.75171\n",
      "Iteration: 02920 - cost: 0.48352 - accuracy: 0.75184\n",
      "Iteration: 02930 - cost: 0.48326 - accuracy: 0.75214\n",
      "Iteration: 02940 - cost: 0.48308 - accuracy: 0.75229\n",
      "Iteration: 02950 - cost: 0.48282 - accuracy: 0.75266\n",
      "Iteration: 02960 - cost: 0.48259 - accuracy: 0.75288\n",
      "Iteration: 02970 - cost: 0.48229 - accuracy: 0.75316\n",
      "Iteration: 02980 - cost: 0.48206 - accuracy: 0.75336\n",
      "Iteration: 02990 - cost: 0.48187 - accuracy: 0.75356\n",
      "Iteration: 03000 - cost: 0.48161 - accuracy: 0.75380\n",
      "Iteration: 03010 - cost: 0.48143 - accuracy: 0.75398\n",
      "Iteration: 03020 - cost: 0.48123 - accuracy: 0.75418\n",
      "Iteration: 03030 - cost: 0.48105 - accuracy: 0.75433\n",
      "Iteration: 03040 - cost: 0.48081 - accuracy: 0.75450\n",
      "Iteration: 03050 - cost: 0.48058 - accuracy: 0.75476\n",
      "Iteration: 03060 - cost: 0.48036 - accuracy: 0.75504\n",
      "Iteration: 03070 - cost: 0.48009 - accuracy: 0.75537\n",
      "Iteration: 03080 - cost: 0.47990 - accuracy: 0.75549\n",
      "Iteration: 03090 - cost: 0.47968 - accuracy: 0.75572\n",
      "Iteration: 03100 - cost: 0.47948 - accuracy: 0.75592\n",
      "Iteration: 03110 - cost: 0.47923 - accuracy: 0.75618\n",
      "Iteration: 03120 - cost: 0.47905 - accuracy: 0.75634\n",
      "Iteration: 03130 - cost: 0.47885 - accuracy: 0.75662\n",
      "Iteration: 03140 - cost: 0.47866 - accuracy: 0.75681\n",
      "Iteration: 03150 - cost: 0.47843 - accuracy: 0.75703\n",
      "Iteration: 03160 - cost: 0.47820 - accuracy: 0.75730\n",
      "Iteration: 03170 - cost: 0.47798 - accuracy: 0.75752\n",
      "Iteration: 03180 - cost: 0.47777 - accuracy: 0.75772\n",
      "Iteration: 03190 - cost: 0.47757 - accuracy: 0.75788\n",
      "Iteration: 03200 - cost: 0.47734 - accuracy: 0.75811\n",
      "Iteration: 03210 - cost: 0.47714 - accuracy: 0.75825\n",
      "Iteration: 03220 - cost: 0.47695 - accuracy: 0.75847\n",
      "Iteration: 03230 - cost: 0.47676 - accuracy: 0.75865\n",
      "Iteration: 03240 - cost: 0.47658 - accuracy: 0.75884\n",
      "Iteration: 03250 - cost: 0.47640 - accuracy: 0.75901\n",
      "Iteration: 03260 - cost: 0.47620 - accuracy: 0.75925\n",
      "Iteration: 03270 - cost: 0.47601 - accuracy: 0.75938\n",
      "Iteration: 03280 - cost: 0.47581 - accuracy: 0.75955\n",
      "Iteration: 03290 - cost: 0.47564 - accuracy: 0.75968\n",
      "Iteration: 03300 - cost: 0.47542 - accuracy: 0.75989\n",
      "Iteration: 03310 - cost: 0.47524 - accuracy: 0.76000\n",
      "Iteration: 03320 - cost: 0.47506 - accuracy: 0.76017\n",
      "Iteration: 03330 - cost: 0.47487 - accuracy: 0.76035\n",
      "Iteration: 03340 - cost: 0.47470 - accuracy: 0.76052\n",
      "Iteration: 03350 - cost: 0.47450 - accuracy: 0.76069\n",
      "Iteration: 03360 - cost: 0.47436 - accuracy: 0.76088\n",
      "Iteration: 03370 - cost: 0.47416 - accuracy: 0.76106\n",
      "Iteration: 03380 - cost: 0.47397 - accuracy: 0.76124\n",
      "Iteration: 03390 - cost: 0.47381 - accuracy: 0.76137\n",
      "Iteration: 03400 - cost: 0.47358 - accuracy: 0.76152\n",
      "Iteration: 03410 - cost: 0.47340 - accuracy: 0.76174\n",
      "Iteration: 03420 - cost: 0.47320 - accuracy: 0.76199\n",
      "Iteration: 03430 - cost: 0.47302 - accuracy: 0.76216\n",
      "Iteration: 03440 - cost: 0.47284 - accuracy: 0.76231\n",
      "Iteration: 03450 - cost: 0.47268 - accuracy: 0.76244\n",
      "Iteration: 03460 - cost: 0.47252 - accuracy: 0.76262\n",
      "Iteration: 03470 - cost: 0.47236 - accuracy: 0.76282\n",
      "Iteration: 03480 - cost: 0.47216 - accuracy: 0.76305\n",
      "Iteration: 03490 - cost: 0.47197 - accuracy: 0.76324\n",
      "Iteration: 03500 - cost: 0.47181 - accuracy: 0.76340\n",
      "Iteration: 03510 - cost: 0.47164 - accuracy: 0.76356\n",
      "Iteration: 03520 - cost: 0.47147 - accuracy: 0.76366\n",
      "Iteration: 03530 - cost: 0.47129 - accuracy: 0.76381\n",
      "Iteration: 03540 - cost: 0.47112 - accuracy: 0.76396\n",
      "Iteration: 03550 - cost: 0.47097 - accuracy: 0.76406\n",
      "Iteration: 03560 - cost: 0.47079 - accuracy: 0.76422\n",
      "Iteration: 03570 - cost: 0.47063 - accuracy: 0.76443\n",
      "Iteration: 03580 - cost: 0.47046 - accuracy: 0.76457\n",
      "Iteration: 03590 - cost: 0.47030 - accuracy: 0.76465\n",
      "Iteration: 03600 - cost: 0.47012 - accuracy: 0.76484\n",
      "Iteration: 03610 - cost: 0.46997 - accuracy: 0.76496\n",
      "Iteration: 03620 - cost: 0.46980 - accuracy: 0.76512\n",
      "Iteration: 03630 - cost: 0.46964 - accuracy: 0.76528\n",
      "Iteration: 03640 - cost: 0.46948 - accuracy: 0.76537\n",
      "Iteration: 03650 - cost: 0.46932 - accuracy: 0.76562\n",
      "Iteration: 03660 - cost: 0.46919 - accuracy: 0.76574\n",
      "Iteration: 03670 - cost: 0.46901 - accuracy: 0.76592\n",
      "Iteration: 03680 - cost: 0.46884 - accuracy: 0.76612\n",
      "Iteration: 03690 - cost: 0.46868 - accuracy: 0.76622\n",
      "Iteration: 03700 - cost: 0.46853 - accuracy: 0.76639\n",
      "Iteration: 03710 - cost: 0.46837 - accuracy: 0.76652\n",
      "Iteration: 03720 - cost: 0.46821 - accuracy: 0.76670\n",
      "Iteration: 03730 - cost: 0.46800 - accuracy: 0.76687\n",
      "Iteration: 03740 - cost: 0.46788 - accuracy: 0.76704\n",
      "Iteration: 03750 - cost: 0.46772 - accuracy: 0.76721\n",
      "Iteration: 03760 - cost: 0.46756 - accuracy: 0.76733\n",
      "Iteration: 03770 - cost: 0.46738 - accuracy: 0.76755\n",
      "Iteration: 03780 - cost: 0.46721 - accuracy: 0.76774\n",
      "Iteration: 03790 - cost: 0.46705 - accuracy: 0.76791\n",
      "Iteration: 03800 - cost: 0.46689 - accuracy: 0.76805\n",
      "Iteration: 03810 - cost: 0.46671 - accuracy: 0.76821\n",
      "Iteration: 03820 - cost: 0.46659 - accuracy: 0.76829\n",
      "Iteration: 03830 - cost: 0.46640 - accuracy: 0.76839\n",
      "Iteration: 03840 - cost: 0.46629 - accuracy: 0.76851\n",
      "Iteration: 03850 - cost: 0.46612 - accuracy: 0.76865\n",
      "Iteration: 03860 - cost: 0.46598 - accuracy: 0.76880\n",
      "Iteration: 03870 - cost: 0.46584 - accuracy: 0.76892\n",
      "Iteration: 03880 - cost: 0.46566 - accuracy: 0.76907\n",
      "Iteration: 03890 - cost: 0.46550 - accuracy: 0.76913\n",
      "Iteration: 03900 - cost: 0.46538 - accuracy: 0.76920\n",
      "Iteration: 03910 - cost: 0.46519 - accuracy: 0.76939\n",
      "Iteration: 03920 - cost: 0.46507 - accuracy: 0.76949\n",
      "Iteration: 03930 - cost: 0.46493 - accuracy: 0.76966\n",
      "Iteration: 03940 - cost: 0.46478 - accuracy: 0.76976\n",
      "Iteration: 03950 - cost: 0.46467 - accuracy: 0.76987\n",
      "Iteration: 03960 - cost: 0.46457 - accuracy: 0.76991\n",
      "Iteration: 03970 - cost: 0.46441 - accuracy: 0.77002\n",
      "Iteration: 03980 - cost: 0.46425 - accuracy: 0.77020\n",
      "Iteration: 03990 - cost: 0.46406 - accuracy: 0.77039\n",
      "Iteration: 04000 - cost: 0.46394 - accuracy: 0.77048\n",
      "Iteration: 04010 - cost: 0.46382 - accuracy: 0.77054\n",
      "Iteration: 04020 - cost: 0.46365 - accuracy: 0.77070\n",
      "Iteration: 04030 - cost: 0.46350 - accuracy: 0.77088\n",
      "Iteration: 04040 - cost: 0.46337 - accuracy: 0.77100\n",
      "Iteration: 04050 - cost: 0.46324 - accuracy: 0.77108\n",
      "Iteration: 04060 - cost: 0.46309 - accuracy: 0.77121\n",
      "Iteration: 04070 - cost: 0.46299 - accuracy: 0.77131\n",
      "Iteration: 04080 - cost: 0.46286 - accuracy: 0.77138\n",
      "Iteration: 04090 - cost: 0.46271 - accuracy: 0.77142\n",
      "Iteration: 04100 - cost: 0.46253 - accuracy: 0.77153\n",
      "Iteration: 04110 - cost: 0.46239 - accuracy: 0.77168\n",
      "Iteration: 04120 - cost: 0.46226 - accuracy: 0.77180\n",
      "Iteration: 04130 - cost: 0.46212 - accuracy: 0.77190\n",
      "Iteration: 04140 - cost: 0.46194 - accuracy: 0.77200\n",
      "Iteration: 04150 - cost: 0.46183 - accuracy: 0.77211\n",
      "Iteration: 04160 - cost: 0.46169 - accuracy: 0.77223\n",
      "Iteration: 04170 - cost: 0.46154 - accuracy: 0.77237\n",
      "Iteration: 04180 - cost: 0.46140 - accuracy: 0.77249\n",
      "Iteration: 04190 - cost: 0.46131 - accuracy: 0.77256\n",
      "Iteration: 04200 - cost: 0.46118 - accuracy: 0.77275\n",
      "Iteration: 04210 - cost: 0.46106 - accuracy: 0.77289\n",
      "Iteration: 04220 - cost: 0.46093 - accuracy: 0.77296\n",
      "Iteration: 04230 - cost: 0.46078 - accuracy: 0.77314\n",
      "Iteration: 04240 - cost: 0.46065 - accuracy: 0.77334\n",
      "Iteration: 04250 - cost: 0.46047 - accuracy: 0.77346\n",
      "Iteration: 04260 - cost: 0.46037 - accuracy: 0.77357\n",
      "Iteration: 04270 - cost: 0.46022 - accuracy: 0.77361\n",
      "Iteration: 04280 - cost: 0.46011 - accuracy: 0.77370\n",
      "Iteration: 04290 - cost: 0.45997 - accuracy: 0.77386\n",
      "Iteration: 04300 - cost: 0.45983 - accuracy: 0.77394\n",
      "Iteration: 04310 - cost: 0.45970 - accuracy: 0.77408\n",
      "Iteration: 04320 - cost: 0.45959 - accuracy: 0.77420\n",
      "Iteration: 04330 - cost: 0.45946 - accuracy: 0.77425\n",
      "Iteration: 04340 - cost: 0.45933 - accuracy: 0.77436\n",
      "Iteration: 04350 - cost: 0.45922 - accuracy: 0.77443\n",
      "Iteration: 04360 - cost: 0.45908 - accuracy: 0.77454\n",
      "Iteration: 04370 - cost: 0.45898 - accuracy: 0.77466\n",
      "Iteration: 04380 - cost: 0.45883 - accuracy: 0.77482\n",
      "Iteration: 04390 - cost: 0.45875 - accuracy: 0.77491\n",
      "Iteration: 04400 - cost: 0.45859 - accuracy: 0.77506\n",
      "Iteration: 04410 - cost: 0.45847 - accuracy: 0.77514\n",
      "Iteration: 04420 - cost: 0.45831 - accuracy: 0.77523\n",
      "Iteration: 04430 - cost: 0.45820 - accuracy: 0.77527\n",
      "Iteration: 04440 - cost: 0.45808 - accuracy: 0.77540\n",
      "Iteration: 04450 - cost: 0.45796 - accuracy: 0.77552\n",
      "Iteration: 04460 - cost: 0.45784 - accuracy: 0.77560\n",
      "Iteration: 04470 - cost: 0.45775 - accuracy: 0.77573\n",
      "Iteration: 04480 - cost: 0.45762 - accuracy: 0.77582\n",
      "Iteration: 04490 - cost: 0.45748 - accuracy: 0.77594\n",
      "Iteration: 04500 - cost: 0.45733 - accuracy: 0.77603\n",
      "Iteration: 04510 - cost: 0.45722 - accuracy: 0.77615\n",
      "Iteration: 04520 - cost: 0.45711 - accuracy: 0.77624\n",
      "Iteration: 04530 - cost: 0.45699 - accuracy: 0.77633\n",
      "Iteration: 04540 - cost: 0.45688 - accuracy: 0.77647\n",
      "Iteration: 04550 - cost: 0.45677 - accuracy: 0.77662\n",
      "Iteration: 04560 - cost: 0.45667 - accuracy: 0.77669\n",
      "Iteration: 04570 - cost: 0.45651 - accuracy: 0.77681\n",
      "Iteration: 04580 - cost: 0.45642 - accuracy: 0.77687\n",
      "Iteration: 04590 - cost: 0.45624 - accuracy: 0.77708\n",
      "Iteration: 04600 - cost: 0.45610 - accuracy: 0.77722\n",
      "Iteration: 04610 - cost: 0.45600 - accuracy: 0.77734\n",
      "Iteration: 04620 - cost: 0.45591 - accuracy: 0.77741\n",
      "Iteration: 04630 - cost: 0.45585 - accuracy: 0.77748\n",
      "Iteration: 04640 - cost: 0.45571 - accuracy: 0.77759\n",
      "Iteration: 04650 - cost: 0.45561 - accuracy: 0.77768\n",
      "Iteration: 04660 - cost: 0.45548 - accuracy: 0.77780\n",
      "Iteration: 04670 - cost: 0.45534 - accuracy: 0.77784\n",
      "Iteration: 04680 - cost: 0.45524 - accuracy: 0.77798\n",
      "Iteration: 04690 - cost: 0.45511 - accuracy: 0.77811\n",
      "Iteration: 04700 - cost: 0.45503 - accuracy: 0.77824\n",
      "Iteration: 04710 - cost: 0.45491 - accuracy: 0.77838\n",
      "Iteration: 04720 - cost: 0.45480 - accuracy: 0.77846\n",
      "Iteration: 04730 - cost: 0.45470 - accuracy: 0.77858\n",
      "Iteration: 04740 - cost: 0.45455 - accuracy: 0.77868\n",
      "Iteration: 04750 - cost: 0.45444 - accuracy: 0.77878\n",
      "Iteration: 04760 - cost: 0.45430 - accuracy: 0.77889\n",
      "Iteration: 04770 - cost: 0.45417 - accuracy: 0.77901\n",
      "Iteration: 04780 - cost: 0.45406 - accuracy: 0.77915\n",
      "Iteration: 04790 - cost: 0.45396 - accuracy: 0.77919\n",
      "Iteration: 04800 - cost: 0.45388 - accuracy: 0.77925\n",
      "Iteration: 04810 - cost: 0.45379 - accuracy: 0.77934\n",
      "Iteration: 04820 - cost: 0.45366 - accuracy: 0.77942\n",
      "Iteration: 04830 - cost: 0.45353 - accuracy: 0.77952\n",
      "Iteration: 04840 - cost: 0.45343 - accuracy: 0.77962\n",
      "Iteration: 04850 - cost: 0.45331 - accuracy: 0.77973\n",
      "Iteration: 04860 - cost: 0.45323 - accuracy: 0.77984\n",
      "Iteration: 04870 - cost: 0.45309 - accuracy: 0.78000\n",
      "Iteration: 04880 - cost: 0.45297 - accuracy: 0.78014\n",
      "Iteration: 04890 - cost: 0.45289 - accuracy: 0.78018\n",
      "Iteration: 04900 - cost: 0.45280 - accuracy: 0.78031\n",
      "Iteration: 04910 - cost: 0.45268 - accuracy: 0.78042\n",
      "Iteration: 04920 - cost: 0.45258 - accuracy: 0.78048\n",
      "Iteration: 04930 - cost: 0.45247 - accuracy: 0.78058\n",
      "Iteration: 04940 - cost: 0.45236 - accuracy: 0.78068\n",
      "Iteration: 04950 - cost: 0.45224 - accuracy: 0.78082\n",
      "Iteration: 04960 - cost: 0.45214 - accuracy: 0.78089\n",
      "Iteration: 04970 - cost: 0.45209 - accuracy: 0.78099\n",
      "Iteration: 04980 - cost: 0.45199 - accuracy: 0.78105\n",
      "Iteration: 04990 - cost: 0.45190 - accuracy: 0.78108\n",
      "Iteration: 05000 - cost: 0.45176 - accuracy: 0.78118\n",
      "Iteration: 05010 - cost: 0.45161 - accuracy: 0.78135\n",
      "Iteration: 05020 - cost: 0.45157 - accuracy: 0.78135\n",
      "Iteration: 05030 - cost: 0.45145 - accuracy: 0.78148\n",
      "Iteration: 05040 - cost: 0.45135 - accuracy: 0.78158\n",
      "Iteration: 05050 - cost: 0.45127 - accuracy: 0.78165\n",
      "Iteration: 05060 - cost: 0.45116 - accuracy: 0.78175\n",
      "Iteration: 05070 - cost: 0.45105 - accuracy: 0.78183\n",
      "Iteration: 05080 - cost: 0.45097 - accuracy: 0.78190\n",
      "Iteration: 05090 - cost: 0.45085 - accuracy: 0.78198\n",
      "Iteration: 05100 - cost: 0.45076 - accuracy: 0.78209\n",
      "Iteration: 05110 - cost: 0.45064 - accuracy: 0.78221\n",
      "Iteration: 05120 - cost: 0.45059 - accuracy: 0.78223\n",
      "Iteration: 05130 - cost: 0.45049 - accuracy: 0.78230\n",
      "Iteration: 05140 - cost: 0.45038 - accuracy: 0.78243\n",
      "Iteration: 05150 - cost: 0.45026 - accuracy: 0.78252\n",
      "Iteration: 05160 - cost: 0.45017 - accuracy: 0.78260\n",
      "Iteration: 05170 - cost: 0.45007 - accuracy: 0.78268\n",
      "Iteration: 05180 - cost: 0.44997 - accuracy: 0.78275\n",
      "Iteration: 05190 - cost: 0.44990 - accuracy: 0.78280\n",
      "Iteration: 05200 - cost: 0.44980 - accuracy: 0.78293\n",
      "Iteration: 05210 - cost: 0.44971 - accuracy: 0.78301\n",
      "Iteration: 05220 - cost: 0.44962 - accuracy: 0.78311\n",
      "Iteration: 05230 - cost: 0.44954 - accuracy: 0.78316\n",
      "Iteration: 05240 - cost: 0.44942 - accuracy: 0.78320\n",
      "Iteration: 05250 - cost: 0.44933 - accuracy: 0.78331\n",
      "Iteration: 05260 - cost: 0.44921 - accuracy: 0.78339\n",
      "Iteration: 05270 - cost: 0.44914 - accuracy: 0.78344\n",
      "Iteration: 05280 - cost: 0.44906 - accuracy: 0.78355\n",
      "Iteration: 05290 - cost: 0.44896 - accuracy: 0.78367\n",
      "Iteration: 05300 - cost: 0.44890 - accuracy: 0.78368\n",
      "Iteration: 05310 - cost: 0.44877 - accuracy: 0.78379\n",
      "Iteration: 05320 - cost: 0.44866 - accuracy: 0.78392\n",
      "Iteration: 05330 - cost: 0.44857 - accuracy: 0.78400\n",
      "Iteration: 05340 - cost: 0.44847 - accuracy: 0.78413\n",
      "Iteration: 05350 - cost: 0.44839 - accuracy: 0.78421\n",
      "Iteration: 05360 - cost: 0.44830 - accuracy: 0.78424\n",
      "Iteration: 05370 - cost: 0.44821 - accuracy: 0.78430\n",
      "Iteration: 05380 - cost: 0.44810 - accuracy: 0.78441\n",
      "Iteration: 05390 - cost: 0.44803 - accuracy: 0.78448\n",
      "Iteration: 05400 - cost: 0.44796 - accuracy: 0.78452\n",
      "Iteration: 05410 - cost: 0.44788 - accuracy: 0.78460\n",
      "Iteration: 05420 - cost: 0.44780 - accuracy: 0.78471\n",
      "Iteration: 05430 - cost: 0.44766 - accuracy: 0.78481\n",
      "Iteration: 05440 - cost: 0.44760 - accuracy: 0.78494\n",
      "Iteration: 05450 - cost: 0.44747 - accuracy: 0.78508\n",
      "Iteration: 05460 - cost: 0.44741 - accuracy: 0.78512\n",
      "Iteration: 05470 - cost: 0.44735 - accuracy: 0.78518\n",
      "Iteration: 05480 - cost: 0.44723 - accuracy: 0.78530\n",
      "Iteration: 05490 - cost: 0.44712 - accuracy: 0.78538\n",
      "Iteration: 05500 - cost: 0.44705 - accuracy: 0.78544\n",
      "Iteration: 05510 - cost: 0.44693 - accuracy: 0.78554\n",
      "Iteration: 05520 - cost: 0.44688 - accuracy: 0.78561\n",
      "Iteration: 05530 - cost: 0.44677 - accuracy: 0.78570\n",
      "Iteration: 05540 - cost: 0.44669 - accuracy: 0.78577\n",
      "Iteration: 05550 - cost: 0.44653 - accuracy: 0.78584\n",
      "Iteration: 05560 - cost: 0.44649 - accuracy: 0.78595\n",
      "Iteration: 05570 - cost: 0.44639 - accuracy: 0.78606\n",
      "Iteration: 05580 - cost: 0.44631 - accuracy: 0.78613\n",
      "Iteration: 05590 - cost: 0.44626 - accuracy: 0.78618\n",
      "Iteration: 05600 - cost: 0.44622 - accuracy: 0.78622\n",
      "Iteration: 05610 - cost: 0.44611 - accuracy: 0.78629\n",
      "Iteration: 05620 - cost: 0.44600 - accuracy: 0.78636\n",
      "Iteration: 05630 - cost: 0.44591 - accuracy: 0.78646\n",
      "Iteration: 05640 - cost: 0.44583 - accuracy: 0.78654\n",
      "Iteration: 05650 - cost: 0.44576 - accuracy: 0.78658\n",
      "Iteration: 05660 - cost: 0.44569 - accuracy: 0.78670\n",
      "Iteration: 05670 - cost: 0.44557 - accuracy: 0.78682\n",
      "Iteration: 05680 - cost: 0.44550 - accuracy: 0.78690\n",
      "Iteration: 05690 - cost: 0.44545 - accuracy: 0.78694\n",
      "Iteration: 05700 - cost: 0.44534 - accuracy: 0.78705\n",
      "Iteration: 05710 - cost: 0.44528 - accuracy: 0.78709\n",
      "Iteration: 05720 - cost: 0.44518 - accuracy: 0.78712\n",
      "Iteration: 05730 - cost: 0.44511 - accuracy: 0.78723\n",
      "Iteration: 05740 - cost: 0.44500 - accuracy: 0.78734\n",
      "Iteration: 05750 - cost: 0.44488 - accuracy: 0.78746\n",
      "Iteration: 05760 - cost: 0.44483 - accuracy: 0.78752\n",
      "Iteration: 05770 - cost: 0.44470 - accuracy: 0.78764\n",
      "Iteration: 05780 - cost: 0.44463 - accuracy: 0.78770\n",
      "Iteration: 05790 - cost: 0.44458 - accuracy: 0.78771\n",
      "Iteration: 05800 - cost: 0.44448 - accuracy: 0.78778\n",
      "Iteration: 05810 - cost: 0.44438 - accuracy: 0.78784\n",
      "Iteration: 05820 - cost: 0.44434 - accuracy: 0.78789\n",
      "Iteration: 05830 - cost: 0.44429 - accuracy: 0.78792\n",
      "Iteration: 05840 - cost: 0.44418 - accuracy: 0.78798\n",
      "Iteration: 05850 - cost: 0.44411 - accuracy: 0.78807\n",
      "Iteration: 05860 - cost: 0.44407 - accuracy: 0.78806\n",
      "Iteration: 05870 - cost: 0.44399 - accuracy: 0.78814\n",
      "Iteration: 05880 - cost: 0.44386 - accuracy: 0.78823\n",
      "Iteration: 05890 - cost: 0.44381 - accuracy: 0.78829\n",
      "Iteration: 05900 - cost: 0.44372 - accuracy: 0.78836\n",
      "Iteration: 05910 - cost: 0.44365 - accuracy: 0.78839\n",
      "Iteration: 05920 - cost: 0.44357 - accuracy: 0.78850\n",
      "Iteration: 05930 - cost: 0.44349 - accuracy: 0.78854\n",
      "Iteration: 05940 - cost: 0.44342 - accuracy: 0.78861\n",
      "Iteration: 05950 - cost: 0.44334 - accuracy: 0.78870\n",
      "Iteration: 05960 - cost: 0.44321 - accuracy: 0.78881\n",
      "Iteration: 05970 - cost: 0.44317 - accuracy: 0.78889\n",
      "Iteration: 05980 - cost: 0.44306 - accuracy: 0.78896\n",
      "Iteration: 05990 - cost: 0.44295 - accuracy: 0.78908\n",
      "Iteration: 06000 - cost: 0.44289 - accuracy: 0.78913\n",
      "Iteration: 06010 - cost: 0.44283 - accuracy: 0.78916\n",
      "Iteration: 06020 - cost: 0.44278 - accuracy: 0.78921\n",
      "Iteration: 06030 - cost: 0.44267 - accuracy: 0.78930\n",
      "Iteration: 06040 - cost: 0.44261 - accuracy: 0.78933\n",
      "Iteration: 06050 - cost: 0.44255 - accuracy: 0.78941\n",
      "Iteration: 06060 - cost: 0.44249 - accuracy: 0.78946\n",
      "Iteration: 06070 - cost: 0.44240 - accuracy: 0.78954\n",
      "Iteration: 06080 - cost: 0.44237 - accuracy: 0.78958\n",
      "Iteration: 06090 - cost: 0.44229 - accuracy: 0.78968\n",
      "Iteration: 06100 - cost: 0.44224 - accuracy: 0.78973\n",
      "Iteration: 06110 - cost: 0.44215 - accuracy: 0.78982\n",
      "Iteration: 06120 - cost: 0.44208 - accuracy: 0.78985\n",
      "Iteration: 06130 - cost: 0.44205 - accuracy: 0.78985\n",
      "Iteration: 06140 - cost: 0.44195 - accuracy: 0.78997\n",
      "Iteration: 06150 - cost: 0.44187 - accuracy: 0.79006\n",
      "Iteration: 06160 - cost: 0.44179 - accuracy: 0.79010\n",
      "Iteration: 06170 - cost: 0.44174 - accuracy: 0.79018\n",
      "Iteration: 06180 - cost: 0.44171 - accuracy: 0.79023\n",
      "Iteration: 06190 - cost: 0.44157 - accuracy: 0.79028\n",
      "Iteration: 06200 - cost: 0.44156 - accuracy: 0.79030\n",
      "Iteration: 06210 - cost: 0.44146 - accuracy: 0.79040\n",
      "Iteration: 06220 - cost: 0.44134 - accuracy: 0.79051\n",
      "Iteration: 06230 - cost: 0.44129 - accuracy: 0.79055\n",
      "Iteration: 06240 - cost: 0.44118 - accuracy: 0.79062\n",
      "Iteration: 06250 - cost: 0.44112 - accuracy: 0.79070\n",
      "Iteration: 06260 - cost: 0.44103 - accuracy: 0.79078\n",
      "Iteration: 06270 - cost: 0.44097 - accuracy: 0.79080\n",
      "Iteration: 06280 - cost: 0.44086 - accuracy: 0.79086\n",
      "Iteration: 06290 - cost: 0.44078 - accuracy: 0.79088\n",
      "Iteration: 06300 - cost: 0.44075 - accuracy: 0.79092\n",
      "Iteration: 06310 - cost: 0.44063 - accuracy: 0.79095\n",
      "Iteration: 06320 - cost: 0.44058 - accuracy: 0.79098\n",
      "Iteration: 06330 - cost: 0.44050 - accuracy: 0.79105\n",
      "Iteration: 06340 - cost: 0.44046 - accuracy: 0.79113\n",
      "Iteration: 06350 - cost: 0.44036 - accuracy: 0.79118\n",
      "Iteration: 06360 - cost: 0.44028 - accuracy: 0.79125\n",
      "Iteration: 06370 - cost: 0.44018 - accuracy: 0.79134\n",
      "Iteration: 06380 - cost: 0.44014 - accuracy: 0.79140\n",
      "Iteration: 06390 - cost: 0.44009 - accuracy: 0.79141\n",
      "Iteration: 06400 - cost: 0.44006 - accuracy: 0.79144\n",
      "Iteration: 06410 - cost: 0.43999 - accuracy: 0.79146\n",
      "Iteration: 06420 - cost: 0.43994 - accuracy: 0.79148\n",
      "Iteration: 06430 - cost: 0.43982 - accuracy: 0.79159\n",
      "Iteration: 06440 - cost: 0.43979 - accuracy: 0.79160\n",
      "Iteration: 06450 - cost: 0.43973 - accuracy: 0.79163\n",
      "Iteration: 06460 - cost: 0.43964 - accuracy: 0.79172\n",
      "Iteration: 06470 - cost: 0.43959 - accuracy: 0.79176\n",
      "Iteration: 06480 - cost: 0.43952 - accuracy: 0.79183\n",
      "Iteration: 06490 - cost: 0.43948 - accuracy: 0.79186\n",
      "Iteration: 06500 - cost: 0.43943 - accuracy: 0.79188\n",
      "Iteration: 06510 - cost: 0.43933 - accuracy: 0.79200\n",
      "Iteration: 06520 - cost: 0.43929 - accuracy: 0.79206\n",
      "Iteration: 06530 - cost: 0.43924 - accuracy: 0.79210\n",
      "Iteration: 06540 - cost: 0.43919 - accuracy: 0.79218\n",
      "Iteration: 06550 - cost: 0.43908 - accuracy: 0.79225\n",
      "Iteration: 06560 - cost: 0.43900 - accuracy: 0.79231\n",
      "Iteration: 06570 - cost: 0.43895 - accuracy: 0.79236\n",
      "Iteration: 06580 - cost: 0.43885 - accuracy: 0.79243\n",
      "Iteration: 06590 - cost: 0.43877 - accuracy: 0.79248\n",
      "Iteration: 06600 - cost: 0.43871 - accuracy: 0.79251\n",
      "Iteration: 06610 - cost: 0.43861 - accuracy: 0.79257\n",
      "Iteration: 06620 - cost: 0.43856 - accuracy: 0.79260\n",
      "Iteration: 06630 - cost: 0.43848 - accuracy: 0.79264\n",
      "Iteration: 06640 - cost: 0.43837 - accuracy: 0.79271\n",
      "Iteration: 06650 - cost: 0.43836 - accuracy: 0.79273\n",
      "Iteration: 06660 - cost: 0.43829 - accuracy: 0.79280\n",
      "Iteration: 06670 - cost: 0.43824 - accuracy: 0.79283\n",
      "Iteration: 06680 - cost: 0.43817 - accuracy: 0.79287\n",
      "Iteration: 06690 - cost: 0.43808 - accuracy: 0.79295\n",
      "Iteration: 06700 - cost: 0.43799 - accuracy: 0.79301\n",
      "Iteration: 06710 - cost: 0.43796 - accuracy: 0.79304\n",
      "Iteration: 06720 - cost: 0.43789 - accuracy: 0.79310\n",
      "Iteration: 06730 - cost: 0.43783 - accuracy: 0.79316\n",
      "Iteration: 06740 - cost: 0.43779 - accuracy: 0.79321\n",
      "Iteration: 06750 - cost: 0.43773 - accuracy: 0.79326\n",
      "Iteration: 06760 - cost: 0.43767 - accuracy: 0.79330\n",
      "Iteration: 06770 - cost: 0.43762 - accuracy: 0.79333\n",
      "Iteration: 06780 - cost: 0.43760 - accuracy: 0.79330\n",
      "Iteration: 06790 - cost: 0.43748 - accuracy: 0.79340\n",
      "Iteration: 06800 - cost: 0.43740 - accuracy: 0.79347\n",
      "Iteration: 06810 - cost: 0.43735 - accuracy: 0.79353\n",
      "Iteration: 06820 - cost: 0.43728 - accuracy: 0.79356\n",
      "Iteration: 06830 - cost: 0.43723 - accuracy: 0.79356\n",
      "Iteration: 06840 - cost: 0.43716 - accuracy: 0.79360\n",
      "Iteration: 06850 - cost: 0.43711 - accuracy: 0.79365\n",
      "Iteration: 06860 - cost: 0.43706 - accuracy: 0.79370\n",
      "Iteration: 06870 - cost: 0.43698 - accuracy: 0.79376\n",
      "Iteration: 06880 - cost: 0.43694 - accuracy: 0.79384\n",
      "Iteration: 06890 - cost: 0.43688 - accuracy: 0.79384\n",
      "Iteration: 06900 - cost: 0.43681 - accuracy: 0.79387\n",
      "Iteration: 06910 - cost: 0.43671 - accuracy: 0.79398\n",
      "Iteration: 06920 - cost: 0.43667 - accuracy: 0.79403\n",
      "Iteration: 06930 - cost: 0.43661 - accuracy: 0.79406\n",
      "Iteration: 06940 - cost: 0.43653 - accuracy: 0.79413\n",
      "Iteration: 06950 - cost: 0.43645 - accuracy: 0.79418\n",
      "Iteration: 06960 - cost: 0.43641 - accuracy: 0.79424\n",
      "Iteration: 06970 - cost: 0.43636 - accuracy: 0.79426\n",
      "Iteration: 06980 - cost: 0.43629 - accuracy: 0.79430\n",
      "Iteration: 06990 - cost: 0.43621 - accuracy: 0.79437\n",
      "Iteration: 07000 - cost: 0.43619 - accuracy: 0.79437\n",
      "Iteration: 07010 - cost: 0.43611 - accuracy: 0.79442\n",
      "Iteration: 07020 - cost: 0.43607 - accuracy: 0.79443\n",
      "Iteration: 07030 - cost: 0.43604 - accuracy: 0.79446\n",
      "Iteration: 07040 - cost: 0.43593 - accuracy: 0.79456\n",
      "Iteration: 07050 - cost: 0.43591 - accuracy: 0.79458\n",
      "Iteration: 07060 - cost: 0.43586 - accuracy: 0.79460\n",
      "Iteration: 07070 - cost: 0.43577 - accuracy: 0.79470\n",
      "Iteration: 07080 - cost: 0.43575 - accuracy: 0.79469\n",
      "Iteration: 07090 - cost: 0.43568 - accuracy: 0.79474\n",
      "Iteration: 07100 - cost: 0.43560 - accuracy: 0.79479\n",
      "Iteration: 07110 - cost: 0.43555 - accuracy: 0.79484\n",
      "Iteration: 07120 - cost: 0.43551 - accuracy: 0.79486\n",
      "Iteration: 07130 - cost: 0.43543 - accuracy: 0.79493\n",
      "Iteration: 07140 - cost: 0.43537 - accuracy: 0.79495\n",
      "Iteration: 07150 - cost: 0.43530 - accuracy: 0.79496\n",
      "Iteration: 07160 - cost: 0.43525 - accuracy: 0.79497\n",
      "Iteration: 07170 - cost: 0.43520 - accuracy: 0.79502\n",
      "Iteration: 07180 - cost: 0.43514 - accuracy: 0.79511\n",
      "Iteration: 07190 - cost: 0.43507 - accuracy: 0.79517\n",
      "Iteration: 07200 - cost: 0.43501 - accuracy: 0.79520\n",
      "Iteration: 07210 - cost: 0.43493 - accuracy: 0.79525\n",
      "Iteration: 07220 - cost: 0.43486 - accuracy: 0.79528\n",
      "Iteration: 07230 - cost: 0.43483 - accuracy: 0.79532\n",
      "Iteration: 07240 - cost: 0.43479 - accuracy: 0.79535\n",
      "Iteration: 07250 - cost: 0.43470 - accuracy: 0.79544\n",
      "Iteration: 07260 - cost: 0.43464 - accuracy: 0.79551\n",
      "Iteration: 07270 - cost: 0.43460 - accuracy: 0.79556\n",
      "Iteration: 07280 - cost: 0.43453 - accuracy: 0.79566\n",
      "Iteration: 07290 - cost: 0.43448 - accuracy: 0.79574\n",
      "Iteration: 07300 - cost: 0.43444 - accuracy: 0.79576\n",
      "Iteration: 07310 - cost: 0.43434 - accuracy: 0.79584\n",
      "Iteration: 07320 - cost: 0.43430 - accuracy: 0.79591\n",
      "Iteration: 07330 - cost: 0.43427 - accuracy: 0.79592\n",
      "Iteration: 07340 - cost: 0.43421 - accuracy: 0.79595\n",
      "Iteration: 07350 - cost: 0.43415 - accuracy: 0.79601\n",
      "Iteration: 07360 - cost: 0.43412 - accuracy: 0.79601\n",
      "Iteration: 07370 - cost: 0.43410 - accuracy: 0.79603\n",
      "Iteration: 07380 - cost: 0.43402 - accuracy: 0.79608\n",
      "Iteration: 07390 - cost: 0.43396 - accuracy: 0.79612\n",
      "Iteration: 07400 - cost: 0.43391 - accuracy: 0.79619\n",
      "Iteration: 07410 - cost: 0.43386 - accuracy: 0.79622\n",
      "Iteration: 07420 - cost: 0.43384 - accuracy: 0.79625\n",
      "Iteration: 07430 - cost: 0.43376 - accuracy: 0.79629\n",
      "Iteration: 07440 - cost: 0.43366 - accuracy: 0.79637\n",
      "Iteration: 07450 - cost: 0.43361 - accuracy: 0.79645\n",
      "Iteration: 07460 - cost: 0.43358 - accuracy: 0.79646\n",
      "Iteration: 07470 - cost: 0.43351 - accuracy: 0.79649\n",
      "Iteration: 07480 - cost: 0.43344 - accuracy: 0.79653\n",
      "Iteration: 07490 - cost: 0.43338 - accuracy: 0.79656\n",
      "Iteration: 07500 - cost: 0.43328 - accuracy: 0.79663\n",
      "Iteration: 07510 - cost: 0.43323 - accuracy: 0.79669\n",
      "Iteration: 07520 - cost: 0.43317 - accuracy: 0.79674\n",
      "Iteration: 07530 - cost: 0.43313 - accuracy: 0.79680\n",
      "Iteration: 07540 - cost: 0.43309 - accuracy: 0.79685\n",
      "Iteration: 07550 - cost: 0.43301 - accuracy: 0.79694\n",
      "Iteration: 07560 - cost: 0.43295 - accuracy: 0.79696\n",
      "Iteration: 07570 - cost: 0.43294 - accuracy: 0.79701\n",
      "Iteration: 07580 - cost: 0.43285 - accuracy: 0.79707\n",
      "Iteration: 07590 - cost: 0.43282 - accuracy: 0.79709\n",
      "Iteration: 07600 - cost: 0.43278 - accuracy: 0.79715\n",
      "Iteration: 07610 - cost: 0.43271 - accuracy: 0.79720\n",
      "Iteration: 07620 - cost: 0.43264 - accuracy: 0.79725\n",
      "Iteration: 07630 - cost: 0.43259 - accuracy: 0.79728\n",
      "Iteration: 07640 - cost: 0.43254 - accuracy: 0.79733\n",
      "Iteration: 07650 - cost: 0.43245 - accuracy: 0.79735\n",
      "Iteration: 07660 - cost: 0.43240 - accuracy: 0.79736\n",
      "Iteration: 07670 - cost: 0.43235 - accuracy: 0.79741\n",
      "Iteration: 07680 - cost: 0.43230 - accuracy: 0.79744\n",
      "Iteration: 07690 - cost: 0.43222 - accuracy: 0.79749\n",
      "Iteration: 07700 - cost: 0.43218 - accuracy: 0.79749\n",
      "Iteration: 07710 - cost: 0.43209 - accuracy: 0.79759\n",
      "Iteration: 07720 - cost: 0.43206 - accuracy: 0.79757\n",
      "Iteration: 07730 - cost: 0.43197 - accuracy: 0.79764\n",
      "Iteration: 07740 - cost: 0.43190 - accuracy: 0.79768\n",
      "Iteration: 07750 - cost: 0.43182 - accuracy: 0.79774\n",
      "Iteration: 07760 - cost: 0.43176 - accuracy: 0.79781\n",
      "Iteration: 07770 - cost: 0.43171 - accuracy: 0.79785\n",
      "Iteration: 07780 - cost: 0.43164 - accuracy: 0.79790\n",
      "Iteration: 07790 - cost: 0.43156 - accuracy: 0.79798\n",
      "Iteration: 07800 - cost: 0.43148 - accuracy: 0.79803\n",
      "Iteration: 07810 - cost: 0.43141 - accuracy: 0.79806\n",
      "Iteration: 07820 - cost: 0.43138 - accuracy: 0.79810\n",
      "Iteration: 07830 - cost: 0.43133 - accuracy: 0.79810\n",
      "Iteration: 07840 - cost: 0.43126 - accuracy: 0.79815\n",
      "Iteration: 07850 - cost: 0.43123 - accuracy: 0.79818\n",
      "Iteration: 07860 - cost: 0.43116 - accuracy: 0.79820\n",
      "Iteration: 07870 - cost: 0.43111 - accuracy: 0.79821\n",
      "Iteration: 07880 - cost: 0.43109 - accuracy: 0.79822\n",
      "Iteration: 07890 - cost: 0.43102 - accuracy: 0.79826\n",
      "Iteration: 07900 - cost: 0.43096 - accuracy: 0.79829\n",
      "Iteration: 07910 - cost: 0.43094 - accuracy: 0.79830\n",
      "Iteration: 07920 - cost: 0.43089 - accuracy: 0.79835\n",
      "Iteration: 07930 - cost: 0.43082 - accuracy: 0.79841\n",
      "Iteration: 07940 - cost: 0.43079 - accuracy: 0.79842\n",
      "Iteration: 07950 - cost: 0.43072 - accuracy: 0.79848\n",
      "Iteration: 07960 - cost: 0.43066 - accuracy: 0.79852\n",
      "Iteration: 07970 - cost: 0.43062 - accuracy: 0.79854\n",
      "Iteration: 07980 - cost: 0.43055 - accuracy: 0.79861\n",
      "Iteration: 07990 - cost: 0.43053 - accuracy: 0.79859\n",
      "Iteration: 08000 - cost: 0.43050 - accuracy: 0.79861\n",
      "Iteration: 08010 - cost: 0.43047 - accuracy: 0.79862\n",
      "Iteration: 08020 - cost: 0.43042 - accuracy: 0.79872\n",
      "Iteration: 08030 - cost: 0.43038 - accuracy: 0.79872\n",
      "Iteration: 08040 - cost: 0.43030 - accuracy: 0.79879\n",
      "Iteration: 08050 - cost: 0.43027 - accuracy: 0.79879\n",
      "Iteration: 08060 - cost: 0.43016 - accuracy: 0.79885\n",
      "Iteration: 08070 - cost: 0.43011 - accuracy: 0.79892\n",
      "Iteration: 08080 - cost: 0.43008 - accuracy: 0.79890\n",
      "Iteration: 08090 - cost: 0.43002 - accuracy: 0.79893\n",
      "Iteration: 08100 - cost: 0.42998 - accuracy: 0.79896\n",
      "Iteration: 08110 - cost: 0.42994 - accuracy: 0.79898\n",
      "Iteration: 08120 - cost: 0.42993 - accuracy: 0.79896\n",
      "Iteration: 08130 - cost: 0.42987 - accuracy: 0.79898\n",
      "Iteration: 08140 - cost: 0.42981 - accuracy: 0.79902\n",
      "Iteration: 08150 - cost: 0.42978 - accuracy: 0.79902\n",
      "Iteration: 08160 - cost: 0.42972 - accuracy: 0.79903\n",
      "Iteration: 08170 - cost: 0.42966 - accuracy: 0.79904\n",
      "Iteration: 08180 - cost: 0.42959 - accuracy: 0.79910\n",
      "Iteration: 08190 - cost: 0.42954 - accuracy: 0.79914\n",
      "Iteration: 08200 - cost: 0.42949 - accuracy: 0.79915\n",
      "Iteration: 08210 - cost: 0.42945 - accuracy: 0.79917\n",
      "Iteration: 08220 - cost: 0.42942 - accuracy: 0.79919\n",
      "Iteration: 08230 - cost: 0.42938 - accuracy: 0.79925\n",
      "Iteration: 08240 - cost: 0.42934 - accuracy: 0.79926\n",
      "Iteration: 08250 - cost: 0.42929 - accuracy: 0.79930\n",
      "Iteration: 08260 - cost: 0.42923 - accuracy: 0.79938\n",
      "Iteration: 08270 - cost: 0.42919 - accuracy: 0.79940\n",
      "Iteration: 08280 - cost: 0.42914 - accuracy: 0.79944\n",
      "Iteration: 08290 - cost: 0.42912 - accuracy: 0.79948\n",
      "Iteration: 08300 - cost: 0.42904 - accuracy: 0.79950\n",
      "Iteration: 08310 - cost: 0.42902 - accuracy: 0.79951\n",
      "Iteration: 08320 - cost: 0.42892 - accuracy: 0.79958\n",
      "Iteration: 08330 - cost: 0.42886 - accuracy: 0.79963\n",
      "Iteration: 08340 - cost: 0.42879 - accuracy: 0.79964\n",
      "Iteration: 08350 - cost: 0.42874 - accuracy: 0.79968\n",
      "Iteration: 08360 - cost: 0.42871 - accuracy: 0.79970\n",
      "Iteration: 08370 - cost: 0.42868 - accuracy: 0.79972\n",
      "Iteration: 08380 - cost: 0.42866 - accuracy: 0.79974\n",
      "Iteration: 08390 - cost: 0.42856 - accuracy: 0.79975\n",
      "Iteration: 08400 - cost: 0.42851 - accuracy: 0.79978\n",
      "Iteration: 08410 - cost: 0.42844 - accuracy: 0.79978\n",
      "Iteration: 08420 - cost: 0.42838 - accuracy: 0.79982\n",
      "Iteration: 08430 - cost: 0.42836 - accuracy: 0.79984\n",
      "Iteration: 08440 - cost: 0.42831 - accuracy: 0.79988\n",
      "Iteration: 08450 - cost: 0.42823 - accuracy: 0.79995\n",
      "Iteration: 08460 - cost: 0.42819 - accuracy: 0.79997\n",
      "Iteration: 08470 - cost: 0.42813 - accuracy: 0.80002\n",
      "Iteration: 08480 - cost: 0.42810 - accuracy: 0.80004\n",
      "Iteration: 08490 - cost: 0.42806 - accuracy: 0.80007\n",
      "Iteration: 08500 - cost: 0.42796 - accuracy: 0.80013\n",
      "Iteration: 08510 - cost: 0.42794 - accuracy: 0.80017\n",
      "Iteration: 08520 - cost: 0.42792 - accuracy: 0.80020\n",
      "Iteration: 08530 - cost: 0.42787 - accuracy: 0.80024\n",
      "Iteration: 08540 - cost: 0.42779 - accuracy: 0.80026\n",
      "Iteration: 08550 - cost: 0.42783 - accuracy: 0.80025\n",
      "Iteration: 08560 - cost: 0.42775 - accuracy: 0.80028\n",
      "Iteration: 08570 - cost: 0.42766 - accuracy: 0.80031\n",
      "Iteration: 08580 - cost: 0.42763 - accuracy: 0.80037\n",
      "Iteration: 08590 - cost: 0.42759 - accuracy: 0.80040\n",
      "Iteration: 08600 - cost: 0.42756 - accuracy: 0.80040\n",
      "Iteration: 08610 - cost: 0.42751 - accuracy: 0.80042\n",
      "Iteration: 08620 - cost: 0.42743 - accuracy: 0.80050\n",
      "Iteration: 08630 - cost: 0.42739 - accuracy: 0.80052\n",
      "Iteration: 08640 - cost: 0.42734 - accuracy: 0.80058\n",
      "Iteration: 08650 - cost: 0.42729 - accuracy: 0.80062\n",
      "Iteration: 08660 - cost: 0.42726 - accuracy: 0.80063\n",
      "Iteration: 08670 - cost: 0.42723 - accuracy: 0.80060\n",
      "Iteration: 08680 - cost: 0.42714 - accuracy: 0.80068\n",
      "Iteration: 08690 - cost: 0.42711 - accuracy: 0.80068\n",
      "Iteration: 08700 - cost: 0.42707 - accuracy: 0.80068\n",
      "Iteration: 08710 - cost: 0.42705 - accuracy: 0.80067\n",
      "Iteration: 08720 - cost: 0.42700 - accuracy: 0.80067\n",
      "Iteration: 08730 - cost: 0.42692 - accuracy: 0.80072\n",
      "Iteration: 08740 - cost: 0.42684 - accuracy: 0.80076\n",
      "Iteration: 08750 - cost: 0.42684 - accuracy: 0.80074\n",
      "Iteration: 08760 - cost: 0.42677 - accuracy: 0.80075\n",
      "Iteration: 08770 - cost: 0.42672 - accuracy: 0.80078\n",
      "Iteration: 08780 - cost: 0.42669 - accuracy: 0.80083\n",
      "Iteration: 08790 - cost: 0.42665 - accuracy: 0.80087\n",
      "Iteration: 08800 - cost: 0.42662 - accuracy: 0.80088\n",
      "Iteration: 08810 - cost: 0.42654 - accuracy: 0.80091\n",
      "Iteration: 08820 - cost: 0.42651 - accuracy: 0.80092\n",
      "Iteration: 08830 - cost: 0.42644 - accuracy: 0.80093\n",
      "Iteration: 08840 - cost: 0.42638 - accuracy: 0.80098\n",
      "Iteration: 08850 - cost: 0.42633 - accuracy: 0.80100\n",
      "Iteration: 08860 - cost: 0.42628 - accuracy: 0.80104\n",
      "Iteration: 08870 - cost: 0.42625 - accuracy: 0.80107\n",
      "Iteration: 08880 - cost: 0.42620 - accuracy: 0.80111\n",
      "Iteration: 08890 - cost: 0.42615 - accuracy: 0.80113\n",
      "Iteration: 08900 - cost: 0.42610 - accuracy: 0.80115\n",
      "Iteration: 08910 - cost: 0.42606 - accuracy: 0.80112\n",
      "Iteration: 08920 - cost: 0.42606 - accuracy: 0.80109\n",
      "Iteration: 08930 - cost: 0.42598 - accuracy: 0.80112\n",
      "Iteration: 08940 - cost: 0.42591 - accuracy: 0.80111\n",
      "Iteration: 08950 - cost: 0.42585 - accuracy: 0.80118\n",
      "Iteration: 08960 - cost: 0.42585 - accuracy: 0.80116\n",
      "Iteration: 08970 - cost: 0.42576 - accuracy: 0.80121\n",
      "Iteration: 08980 - cost: 0.42573 - accuracy: 0.80129\n",
      "Iteration: 08990 - cost: 0.42570 - accuracy: 0.80126\n",
      "Iteration: 09000 - cost: 0.42565 - accuracy: 0.80127\n",
      "Iteration: 09010 - cost: 0.42564 - accuracy: 0.80128\n",
      "Iteration: 09020 - cost: 0.42557 - accuracy: 0.80129\n",
      "Iteration: 09030 - cost: 0.42550 - accuracy: 0.80133\n",
      "Iteration: 09040 - cost: 0.42545 - accuracy: 0.80133\n",
      "Iteration: 09050 - cost: 0.42543 - accuracy: 0.80135\n",
      "Iteration: 09060 - cost: 0.42537 - accuracy: 0.80137\n",
      "Iteration: 09070 - cost: 0.42530 - accuracy: 0.80142\n",
      "Iteration: 09080 - cost: 0.42527 - accuracy: 0.80142\n",
      "Iteration: 09090 - cost: 0.42524 - accuracy: 0.80145\n",
      "Iteration: 09100 - cost: 0.42520 - accuracy: 0.80147\n",
      "Iteration: 09110 - cost: 0.42512 - accuracy: 0.80150\n",
      "Iteration: 09120 - cost: 0.42509 - accuracy: 0.80152\n",
      "Iteration: 09130 - cost: 0.42503 - accuracy: 0.80150\n",
      "Iteration: 09140 - cost: 0.42500 - accuracy: 0.80148\n",
      "Iteration: 09150 - cost: 0.42496 - accuracy: 0.80151\n",
      "Iteration: 09160 - cost: 0.42491 - accuracy: 0.80154\n",
      "Iteration: 09170 - cost: 0.42483 - accuracy: 0.80159\n",
      "Iteration: 09180 - cost: 0.42480 - accuracy: 0.80164\n",
      "Iteration: 09190 - cost: 0.42477 - accuracy: 0.80167\n",
      "Iteration: 09200 - cost: 0.42470 - accuracy: 0.80174\n",
      "Iteration: 09210 - cost: 0.42469 - accuracy: 0.80176\n",
      "Iteration: 09220 - cost: 0.42462 - accuracy: 0.80179\n",
      "Iteration: 09230 - cost: 0.42458 - accuracy: 0.80183\n",
      "Iteration: 09240 - cost: 0.42454 - accuracy: 0.80185\n",
      "Iteration: 09250 - cost: 0.42450 - accuracy: 0.80188\n",
      "Iteration: 09260 - cost: 0.42444 - accuracy: 0.80191\n",
      "Iteration: 09270 - cost: 0.42441 - accuracy: 0.80191\n",
      "Iteration: 09280 - cost: 0.42436 - accuracy: 0.80193\n",
      "Iteration: 09290 - cost: 0.42427 - accuracy: 0.80200\n",
      "Iteration: 09300 - cost: 0.42428 - accuracy: 0.80197\n",
      "Iteration: 09310 - cost: 0.42422 - accuracy: 0.80202\n",
      "Iteration: 09320 - cost: 0.42417 - accuracy: 0.80206\n",
      "Iteration: 09330 - cost: 0.42412 - accuracy: 0.80210\n",
      "Iteration: 09340 - cost: 0.42403 - accuracy: 0.80216\n",
      "Iteration: 09350 - cost: 0.42402 - accuracy: 0.80219\n",
      "Iteration: 09360 - cost: 0.42395 - accuracy: 0.80219\n",
      "Iteration: 09370 - cost: 0.42389 - accuracy: 0.80224\n",
      "Iteration: 09380 - cost: 0.42387 - accuracy: 0.80224\n",
      "Iteration: 09390 - cost: 0.42387 - accuracy: 0.80224\n",
      "Iteration: 09400 - cost: 0.42376 - accuracy: 0.80233\n",
      "Iteration: 09410 - cost: 0.42372 - accuracy: 0.80235\n",
      "Iteration: 09420 - cost: 0.42372 - accuracy: 0.80236\n",
      "Iteration: 09430 - cost: 0.42362 - accuracy: 0.80242\n",
      "Iteration: 09440 - cost: 0.42359 - accuracy: 0.80243\n",
      "Iteration: 09450 - cost: 0.42353 - accuracy: 0.80245\n",
      "Iteration: 09460 - cost: 0.42350 - accuracy: 0.80251\n",
      "Iteration: 09470 - cost: 0.42340 - accuracy: 0.80261\n",
      "Iteration: 09480 - cost: 0.42339 - accuracy: 0.80258\n",
      "Iteration: 09490 - cost: 0.42330 - accuracy: 0.80268\n",
      "Iteration: 09500 - cost: 0.42330 - accuracy: 0.80265\n",
      "Iteration: 09510 - cost: 0.42320 - accuracy: 0.80272\n",
      "Iteration: 09520 - cost: 0.42320 - accuracy: 0.80272\n",
      "Iteration: 09530 - cost: 0.42316 - accuracy: 0.80272\n",
      "Iteration: 09540 - cost: 0.42310 - accuracy: 0.80272\n",
      "Iteration: 09550 - cost: 0.42299 - accuracy: 0.80281\n",
      "Iteration: 09560 - cost: 0.42297 - accuracy: 0.80282\n",
      "Iteration: 09570 - cost: 0.42289 - accuracy: 0.80286\n",
      "Iteration: 09580 - cost: 0.42286 - accuracy: 0.80286\n",
      "Iteration: 09590 - cost: 0.42281 - accuracy: 0.80286\n",
      "Iteration: 09600 - cost: 0.42276 - accuracy: 0.80291\n",
      "Iteration: 09610 - cost: 0.42270 - accuracy: 0.80293\n",
      "Iteration: 09620 - cost: 0.42261 - accuracy: 0.80295\n",
      "Iteration: 09630 - cost: 0.42264 - accuracy: 0.80295\n",
      "Iteration: 09640 - cost: 0.42263 - accuracy: 0.80296\n",
      "Iteration: 09650 - cost: 0.42257 - accuracy: 0.80301\n",
      "Iteration: 09660 - cost: 0.42253 - accuracy: 0.80302\n",
      "Iteration: 09670 - cost: 0.42250 - accuracy: 0.80303\n",
      "Iteration: 09680 - cost: 0.42245 - accuracy: 0.80304\n",
      "Iteration: 09690 - cost: 0.42243 - accuracy: 0.80304\n",
      "Iteration: 09700 - cost: 0.42240 - accuracy: 0.80310\n",
      "Iteration: 09710 - cost: 0.42233 - accuracy: 0.80313\n",
      "Iteration: 09720 - cost: 0.42228 - accuracy: 0.80317\n",
      "Iteration: 09730 - cost: 0.42224 - accuracy: 0.80320\n",
      "Iteration: 09740 - cost: 0.42221 - accuracy: 0.80326\n",
      "Iteration: 09750 - cost: 0.42219 - accuracy: 0.80327\n",
      "Iteration: 09760 - cost: 0.42215 - accuracy: 0.80326\n",
      "Iteration: 09770 - cost: 0.42213 - accuracy: 0.80324\n",
      "Iteration: 09780 - cost: 0.42207 - accuracy: 0.80324\n",
      "Iteration: 09790 - cost: 0.42201 - accuracy: 0.80331\n",
      "Iteration: 09800 - cost: 0.42192 - accuracy: 0.80340\n",
      "Iteration: 09810 - cost: 0.42194 - accuracy: 0.80337\n",
      "Iteration: 09820 - cost: 0.42184 - accuracy: 0.80342\n",
      "Iteration: 09830 - cost: 0.42186 - accuracy: 0.80340\n",
      "Iteration: 09840 - cost: 0.42176 - accuracy: 0.80344\n",
      "Iteration: 09850 - cost: 0.42175 - accuracy: 0.80344\n",
      "Iteration: 09860 - cost: 0.42170 - accuracy: 0.80344\n",
      "Iteration: 09870 - cost: 0.42164 - accuracy: 0.80348\n",
      "Iteration: 09880 - cost: 0.42163 - accuracy: 0.80346\n",
      "Iteration: 09890 - cost: 0.42159 - accuracy: 0.80348\n",
      "Iteration: 09900 - cost: 0.42152 - accuracy: 0.80352\n",
      "Iteration: 09910 - cost: 0.42148 - accuracy: 0.80355\n",
      "Iteration: 09920 - cost: 0.42145 - accuracy: 0.80356\n",
      "Iteration: 09930 - cost: 0.42142 - accuracy: 0.80361\n",
      "Iteration: 09940 - cost: 0.42132 - accuracy: 0.80365\n",
      "Iteration: 09950 - cost: 0.42129 - accuracy: 0.80366\n",
      "Iteration: 09960 - cost: 0.42123 - accuracy: 0.80372\n",
      "Iteration: 09970 - cost: 0.42118 - accuracy: 0.80379\n",
      "Iteration: 09980 - cost: 0.42117 - accuracy: 0.80379\n",
      "Iteration: 09990 - cost: 0.42109 - accuracy: 0.80383\n"
     ]
    }
   ],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": len(tX[0]), \"output_dim\": 32, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 32, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 128, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "y[y == -1] = 0\n",
    "inds = np.where(y == 0)[0]\n",
    "np.random.shuffle(inds)\n",
    "size = inds.shape[0]\n",
    "# copy = np.copy(inds)\n",
    "# np.random.shuffle(inds)\n",
    "# print(np.equal(inds, copy))\n",
    "y_balanced = np.delete(y, inds[:size - len(y[y == 1])])\n",
    "\n",
    "tX_balanced = np.delete(tX, inds[:size - len(y[y == 1])], 0)\n",
    "\n",
    "p = np.random.permutation(len(tX))\n",
    "params_values = train(np.transpose(tX), np.transpose(y.reshape((-1 , 1))), NN_ARCHITECTURE, 10000, 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12541e+02  4.64670e+01  7.37400e+01  3.84720e+01  2.09900e+00\n",
      "  2.26345e+02 -2.44000e-01  2.49200e+00  1.24130e+01  1.20666e+02\n",
      "  1.28200e+00 -3.56000e-01  4.56000e-01  3.17655e+01 -2.20000e-02\n",
      " -4.20000e-02  4.05530e+01 -3.80000e-02  9.70000e-02  3.47540e+01\n",
      " -1.60000e-02  1.79940e+02  1.00000e+00  6.58390e+01  1.00000e-03\n",
      " -4.80000e-02  4.80370e+01 -1.20000e-02 -2.70000e-02  4.05040e+01]\n",
      "[ 1.20451557e+02  4.92583872e+01  8.11223377e+01  5.78290937e+01\n",
      "  2.18809638e+00  2.68855809e+02 -4.15146982e-01  2.37421083e+00\n",
      "  1.89926203e+01  1.58668286e+02  1.43928858e+00 -1.26825167e-01\n",
      "  4.56746303e-01  3.86940752e+01 -1.19663944e-02 -1.53522257e-02\n",
      "  4.67065833e+01 -1.88911706e-02  5.20638271e-02  4.16269376e+01\n",
      " -7.98097804e-03  2.09957809e+02  9.80251233e-01  7.72944149e+01\n",
      "  1.87115258e-04 -3.22294215e-02  5.08991836e+01 -9.78708569e-03\n",
      " -2.33218123e-02  7.32676287e+01]\n"
     ]
    }
   ],
   "source": [
    "y_test, tX_test, ids = load_csv_data('data/test.csv')\n",
    "\n",
    "inds = np.where(tX_test == -999)\n",
    "tX_test[inds] = np.nan\n",
    "\n",
    "col_mean = np.nanmedian(tX_test, axis=0)\n",
    "print(col_mean)\n",
    "\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(tX_test))\n",
    "\n",
    "#Place column means in the indices. Align the arrays using ta`ke\n",
    "tX_test[inds] = np.take(col_mean, inds[1])\n",
    "print(np.nanmean(tX_test, axis=0))\n",
    "\n",
    "xmin, xmax = np.min(tX_test, axis=0), np.max(tX_test, axis=0)\n",
    "tX_test = (tX_test - xmin) / (xmax-xmin)\n",
    "\n",
    "Y_test_pred, _ = full_forward_propagation(np.transpose(tX_test), params_values, NN_ARCHITECTURE)\n",
    "Y_test_pred[Y_test_pred == 0] = -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. ... -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import create_csv_submission\n",
    "\n",
    "print(Y_test_pred)\n",
    "create_csv_submission(ids, Y_test_pred[0], 'data/nn.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a_file = open(\"data/weights.pkl\", \"wb\")\n",
    "pickle.dump(params_values, a_file)\n",
    "a_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}