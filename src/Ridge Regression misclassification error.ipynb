{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data_path = '../data/train.csv'\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == 'b')] = -1\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "y, x, ids = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.124060e+02  4.652400e+01  7.375200e+01  3.846750e+01  2.107000e+00\n",
      "  2.258850e+02 -2.440000e-01  2.491500e+00  1.231550e+01  1.206645e+02\n",
      "  1.280000e+00 -3.560000e-01  4.540000e-01  3.180400e+01 -2.300000e-02\n",
      " -3.300000e-02  4.051600e+01 -4.500000e-02  8.600000e-02  3.480200e+01\n",
      " -2.400000e-02  1.797390e+02  1.000000e+00  6.556100e+01  0.000000e+00\n",
      " -3.300000e-02  4.790200e+01 -1.000000e-02 -2.000000e-03  4.051250e+01]\n",
      "[ 1.20417434e+02  4.92398193e+01  8.11819816e+01  5.78959617e+01\n",
      "  2.19310420e+00  2.68220619e+02 -4.11628932e-01  2.37309984e+00\n",
      "  1.89173324e+01  1.58432217e+02  1.43760943e+00 -1.28304708e-01\n",
      "  4.55244780e-01  3.87074191e+01 -1.09730480e-02 -8.17107200e-03\n",
      "  4.66602072e+01 -1.95074680e-02  4.35429640e-02  4.17172345e+01\n",
      " -1.01191920e-02  2.09797178e+02  9.79176000e-01  7.71243656e+01\n",
      " -1.96589200e-03 -2.06285240e-02  5.07391493e+01 -1.05354440e-02\n",
      " -1.87879200e-03  7.30645914e+01]\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(x == -999)\n",
    "x[inds] = np.nan\n",
    "\n",
    "col_mean = np.nanmedian(x, axis=0)\n",
    "print(col_mean)\n",
    "\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(x))\n",
    "\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "x[inds] = np.take(col_mean, inds[1])\n",
    "print(np.nanmean(x, axis=0))\n",
    "\n",
    "xmin, xmax = np.min(x, axis=0), np.max(x, axis=0)\n",
    "x = (x - xmin) / (xmax-xmin)\n",
    "# xmean, xstd = np.mean(x, axis=0), np.std(x, axis=0)\n",
    "# x = (x - xmean) / xstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with mean of col\n",
    "# col_mean = np.nanmean(x, axis=0)\n",
    "# inds = np.where(x==-999)\n",
    "# x[inds] = np.take(col_mean, inds[1])\n",
    "#\n",
    "# minmax normalize\n",
    "# xmin, xmax = np.min(x, axis=0), np.max(x, axis=0)\n",
    "# x = (x - xmin) / (xmax-xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    e = y-tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n",
    "\n",
    "def build_poly(x, degree):\n",
    "\n",
    "    \"\"\"\n",
    "    Builds polynomial augmented dataset\n",
    "    :param x: \n",
    "    :param degree: \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r = x.copy()\n",
    "    for deg in range (2,degree+1):\n",
    "        r = np.c_[r, np.power(x, deg)]\n",
    "        \n",
    "    return np.c_[np.ones(r.shape[0]), r]\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row/k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k*interval: (k+1) * interval] for k in range (k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1e-20, degree 13\n",
      "0.18084144673157385\n"
     ]
    }
   ],
   "source": [
    "# try training with different lambdas and refine the lambdas depending on what magnitude turns out to be best\n",
    "# same idea with degrees\n",
    "import pickle\n",
    "\n",
    "lambdas = np.array([1e-20])\n",
    "degrees = np.array([13])\n",
    "seed = 12\n",
    "k_fold = 7\n",
    "\n",
    "def predict(tx, w):\n",
    "    return tx.dot(w)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te, y_tr = y[te_indice], y[tr_indice]\n",
    "    x_te, x_tr = x[te_indice], x[tr_indice]\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    initial_w = np.ones(tx_tr.shape[1])\n",
    "    \n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    \n",
    "    y_tr_pred = predict(tx_tr, w)\n",
    "    y_te_pred = predict(tx_te, w)\n",
    "    \n",
    "    y_tr_pred[np.where(y_tr_pred <= 0)] = -1\n",
    "    y_tr_pred[np.where(y_tr_pred > 0)] = 1\n",
    "    \n",
    "    y_te_pred[np.where(y_te_pred <= 0)] = -1\n",
    "    y_te_pred[np.where(y_te_pred > 0)] = 1\n",
    "\n",
    "    loss_tr = sum(y_tr_pred != y_tr)/len(y_tr)\n",
    "    loss_te = sum(y_te_pred != y_te)/len(y_te)\n",
    "    \n",
    "    return loss_tr, loss_te, w\n",
    "\n",
    "def cross_validation_demo():\n",
    "   \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    best_lambdas = []\n",
    "    best_losses = []\n",
    "    best_weights = []\n",
    "    best_losses_tr = []\n",
    "    \n",
    "    j = 0 #jth degree\n",
    "    for deg in degrees:\n",
    "        i = 0 #ith lambda\n",
    "        losses_te = []\n",
    "        losses_tr = []\n",
    "        ws = []\n",
    "        for lambda_ in lambdas:\n",
    "            print(f'Lambda {lambdas[i]}, degree {degrees[j]}')\n",
    "            i += 1\n",
    "            ws_tmp = []\n",
    "            loss_te_tmp = []\n",
    "            loss_tr_tmp = []\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                #print(f'Fold {k}/{k_fold}')\n",
    "                loss_tr, loss_te, w = cross_validation(y, x, k_indices, k, lambda_, deg)\n",
    "                loss_te_tmp.append(loss_te)\n",
    "                loss_tr_tmp.append(loss_tr)\n",
    "                ws_tmp.append(w)\n",
    "            losses_te.append(np.mean(loss_te_tmp, axis=0))\n",
    "            losses_tr.append(np.mean(loss_tr_tmp, axis=0))\n",
    "            ws.append(np.mean(ws_tmp, axis=0))\n",
    "            \n",
    "            def predict(tx, w):\n",
    "                return tx.dot(w)\n",
    "\n",
    "            print(np.mean(loss_te_tmp, axis=0))\n",
    "\n",
    "        j += 1\n",
    "        ind_lambda_opt = np.argmin(loss_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_losses.append(losses_te[ind_lambda_opt])\n",
    "        best_losses_tr.append(losses_tr[ind_lambda_opt])\n",
    "        best_weights.append(ws[ind_lambda_opt])\n",
    "        \n",
    "        with open('../../vars_ridge.pkl', 'wb') as f:\n",
    "            pickle.dump([best_lambdas,best_losses,best_losses_tr,best_weights], f)\n",
    "        \n",
    "\n",
    "    ind_best_degree = np.argmin(best_losses)\n",
    "\n",
    "    return degrees[ind_best_degree], best_lambdas, best_weights, best_losses, best_losses_tr\n",
    "\n",
    "best_degree, best_lambdas_per_degree, best_weights_per_degree, best_losses, best_losses_tr = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-20]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambdas_per_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18084144673157385]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18022677514753452]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_losses_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204919"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(tx, w):\n",
    "    return tx.dot(w)\n",
    "\n",
    "y_pred = predict(build_poly(x, best_degree), best_weights_per_degree[0])\n",
    "y_pred[np.where(y_pred <= 0)] = -1\n",
    "y_pred[np.where(y_pred > 0)] = 1\n",
    "\n",
    "sum(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../data/test.csv'\n",
    "\n",
    "y, x, ids = load_csv_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with mean of col\n",
    "inds = np.where(x==-999)\n",
    "x[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "# minmax normalize\n",
    "x = (x - xmin) / (xmax-xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "            \n",
    "y_pred = predict(build_poly(x, 13), best_weights_per_degree[0])\n",
    "y_pred[np.where(y_pred <= 0)] = -1\n",
    "y_pred[np.where(y_pred > 0)] = 1\n",
    "\n",
    "create_csv_submission(ids, y_pred, 'ridge_deg13_1e-20_minmax_median.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
