{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising our approach to tuning Ridge Regression\n",
    "\n",
    "This notebook serves the purpose of presenting our key findings while tuning our ridge regression's parameters and showing how we set out to find these parameters, namely determining the lambda used for regularization and the degrees to which we augment our data. Further, we decide to illustrate why we decided to prioritise lower misclassification in determining these parameters than mean squared error, and reinforce what was taught to us, that the mean squared error can mislead if blindly trusted in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import build_poly, build_k_indices, fill_nan_closure, minmax_normalize_closure, standardize_closure, batch_iter\n",
    "from proj1_helpers import load_csv_data, predict_labels\n",
    "from implementations import compute_mse, ridge_regression \n",
    "import pickle # Use of pickle to bypass long computation which is for vizualisation purposes\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "seed = 12\n",
    "\n",
    "# We work with the training data in this notebook\n",
    "y, x, ids = load_csv_data(DATA_PATH+'train.csv')\n",
    "\n",
    "# Fill all NA values (-999 in this case) with the median of each column - will compare later median vs mean but not now\n",
    "x, _ = fill_nan_closure(x, np.nanmedian)\n",
    "\n",
    "# Minmax normalization of x matrix - will compare minmax vs standardization later in this notebook but not now\n",
    "minmax_normalize = minmax_normalize_closure(np.min(x, axis=0), np.max(x, axis=0))\n",
    "x = minmax_normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.array([1e-5]) #np.logspace(-10,5,5)\n",
    "degrees = (1,2) \n",
    "k_fold = 7\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \n",
    "    # Current fold test indices are our k indices, all non-k indices are our train indices\n",
    "    te_indice, tr_indice = k_indices[k], k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    \n",
    "    # Split data according to determined indices\n",
    "    y_te, y_tr = y[te_indice], y[tr_indice]\n",
    "    x_te, x_tr = x[te_indice], x[tr_indice]\n",
    "    \n",
    "    # Build tildeX (including offset term) and augment to provided degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    \n",
    "    # Compute optimal weights using normal form solution\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    \n",
    "    # Calculate fold MSE for training and test partitions\n",
    "    mse_tr = compute_mse(y_tr, tx_tr, w)\n",
    "    mse_te = compute_mse(y_te, tx_te, w)\n",
    "    \n",
    "    # Calculate fold misclassification % (ratio of how many inaccurate predictions were made)\n",
    "    y_tr_pred = predict_labels(tx_tr, w)\n",
    "    y_te_pred = predict_labels(tx_te, w) # we store the predictions on test to show what may drive mse losses in each degree \n",
    "\n",
    "    misclass_tr = sum(y_tr_pred != y_tr)/len(y_tr)\n",
    "    misclass_te = sum(y_te_pred != y_te)/len(y_te)\n",
    "    \n",
    "    return mse_tr, mse_te, misclass_tr, misclass_te, y_te_pred, w\n",
    "\n",
    "def compare_mse_misclassification_tuning():\n",
    "   \n",
    "    # build fold indices to feed into cross validation\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    # initiate empty lists to store \"best\" lambdas and their corresponding average (over the folds)\n",
    "    # mse on training and test and average misclassification on training on test\n",
    "    # Note that we say \"best\" which here we determine by the lowest mse, which is what we want to show is \n",
    "    # problematic in the case of classification\n",
    "    \n",
    "    best_lambdas = []\n",
    "    best_mses_te, best_mses_tr = [], []\n",
    "    best_acc_te, best_acc_tr = [], []\n",
    "    best_y_pred = []\n",
    "    \n",
    "    for deg in degrees:\n",
    "        print(f'Computing for Degree {deg}')\n",
    "        \n",
    "        # store mses and misclassification % for all lambdas in the given degree\n",
    "        mse_te, mse_tr = [], []\n",
    "        acc_te, acc_tr = [], []\n",
    "        y_pred_te = []\n",
    "\n",
    "        # main reason we decide to loop over some lambdas is to optimise towards the mse and give it its \"best shot\"\n",
    "        for lambda_ in lambdas:\n",
    "\n",
    "            mse_te_tmp, mse_tr_tmp = [], []\n",
    "            acc_te_tmp, acc_tr_tmp = [], []\n",
    "            y_pred_te_tmp = [] \n",
    "            \n",
    "            for k in range(k_fold):\n",
    "\n",
    "                fold_mse_tr, fold_mse_te, fold_acc_tr, fold_acc_te, y_te_pred , _ = cross_validation(y, x, k_indices, k, lambda_, deg)\n",
    "                \n",
    "                # store mse and misclassification accuracy of fold for logging\n",
    "                mse_te_tmp.append(fold_mse_te) ; mse_tr_tmp.append(fold_mse_tr)\n",
    "                acc_te_tmp.append(fold_acc_te) ; acc_tr_tmp.append(fold_acc_tr)\n",
    "                \n",
    "            # average the folds and store for logging    \n",
    "            mse_te.append(np.mean(mse_te_tmp, axis=0)) ; mse_tr.append(np.mean(mse_tr_tmp, axis=0))\n",
    "            acc_te.append(np.mean(acc_te_tmp, axis=0)) ; acc_tr.append(np.mean(acc_tr_tmp, axis=0))\n",
    "            \n",
    "        # append only the mse and misclassification % for the lowest mse - this is our \"best\" lambda\n",
    "        # we will use this to show why perhaps we want to move to minimizing the misclassification to select our best model\n",
    "        ind_lambda_opt = np.argmin(mse_te)\n",
    "        \n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_mses_te.append(mse_te[ind_lambda_opt]) ; best_mses_tr.append(mse_tr[ind_lambda_opt])\n",
    "        best_acc_te.append(acc_te[ind_lambda_opt]) ; best_acc_tr.append(acc_tr[ind_lambda_opt])\n",
    "\n",
    "        # save progressively the values to later avoid re-computing\n",
    "        with open('../../comparing mse and misclassification tuning.pkl', 'wb') as f:\n",
    "            pickle.dump([best_lambdas, best_mses_te, best_mses_tr, best_acc_te, best_acc_tr], f)\n",
    "        \n",
    "    best_d_mse = degrees[np.argmin(best_mses_te)]\n",
    "    best_d_misclass = degrees[np.argmin(acc_te)]\n",
    "\n",
    "    # We recognize we selected the \"best\" values for each degree according to MSE (instead of misclassification %) but it still shows why MSE is flawed\n",
    "    print(f'According to the lowest MSE the best degree was {best_d_mse}, while according to the lowest misclassifications it was {best_d_misclass}')\n",
    "    \n",
    "    return best_lambdas, best_mses_te, best_mses_tr, best_acc_te, best_acc_tr\n",
    "\n",
    "best_lambdas, best_mses_te, best_mses_tr, best_acc_te, best_acc_tr = compare_mse_misclassification_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
