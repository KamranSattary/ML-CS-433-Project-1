{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_dataset='./data/'\n",
    "standardize = False\n",
    "min_max_scale = True\n",
    "master = np.genfromtxt(path_dataset+'train.csv', delimiter=\",\", skip_header=1,\n",
    "                       converters={1: lambda x: float(0) if b\"b\" in x else float(1)})\n",
    "train_y, train_x = master[:, 1], np.delete(master, [0, 1], axis=1)\n",
    "\n",
    "test_x = np.genfromtxt(path_dataset+'test.csv', delimiter=\",\", skip_header=1)\n",
    "test_x = np.delete(test_x, [0, 1], axis=1)\n",
    "\n",
    "if standardize == True:\n",
    "    xmean, xstd = np.mean(train_x, axis=0), np.std(train_x, axis=0)\n",
    "    train_x = (train_x - xmean) / xstd\n",
    "    test_x = (test_x - xmean) / xstd\n",
    "    \n",
    "if min_max_scale == True:\n",
    "    xmin, xmax = np.min(train_x, axis=0), np.max(train_x, axis=0)\n",
    "    train_x = (train_x - xmin) / (xmax-xmin)\n",
    "    test_x = (test_x - xmin) / (xmax-xmin)\n",
    "    \n",
    "y, x = train_y, train_x\n",
    "\n",
    "def build_poly(x, degree):\n",
    "\n",
    "    \"\"\"\n",
    "    Builds polynomial augmented dataset\n",
    "    :param x: \n",
    "    :param degree: \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r = x.copy()\n",
    "    for deg in range (2,degree+1):\n",
    "        r = np.c_[r, np.power(x, deg)]\n",
    "        \n",
    "    return np.c_[np.ones(r.shape[0]), r]\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row/k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k*interval: (k+1) * interval] for k in range (k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigmoid(xw):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes sigmoid of input vector\n",
    "    :param xw: (n,) array input to be sigmoid transformed\n",
    "    :return: (n,) sigmoid-transformed vector\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-xw))\n",
    "\n",
    "def nlog_likelihood(y, tx, w):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood loss\n",
    "    :param y: (n,) array\n",
    "    :param tx: (n,d) matrix\n",
    "    :param w: (d,) array of weights\n",
    "    :return: computed loss given by negative log likelihood\n",
    "    \"\"\"\n",
    "    pred = compute_sigmoid(tx.dot(w))\n",
    "    pred = np.where(pred == 0, 0.000001, pred)\n",
    "    pred = np.where(pred == 1, 0.999999, pred)\n",
    "    loss = y.T.dot(np.log(pred)) + (1-y).T.dot(np.log(1-pred))\n",
    "    return -loss\n",
    "\n",
    "def compute_gradient_sigmoid(y, tx, w):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute sigmoid gradient\n",
    "    :param y: (n,) array\n",
    "    :param tx: (n,d) matrix\n",
    "    :param w: (d,) array of initial weights\n",
    "    :return: (d,) array of computed gradient vector\n",
    "    \"\"\"\n",
    "\n",
    "    pred = compute_sigmoid(tx.dot(w))\n",
    "    grd = tx.T.dot(pred-y)\n",
    "\n",
    "    return grd\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gammas, break_threshold):\n",
    "\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using SGD\n",
    "    :param y: (n,) array\n",
    "    :param tx: (n,d) matrix\n",
    "    :param intial_w: (d,) array of initial weights\n",
    "    :param max_iters: int indicating maximum iterations\n",
    "    :param gamma: float indicating learning rate\n",
    "    :return: optimal weights vector, loss(mse)\n",
    "    \"\"\"\n",
    "\n",
    "    w = initial_w\n",
    "    # uniform picking of minibatch of a single datapoint in this case\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            break_count = 0\n",
    "            \n",
    "            # progressively reduce gamma size \n",
    "            if n_iter < 100: \n",
    "                gamma = gammas[0]\n",
    "            elif n_iter < 750:\n",
    "                gamma = gammas[1]\n",
    "            elif n_iter < 1000:\n",
    "                gamma = gammas[2]\n",
    "            else:\n",
    "                gamma = gammas[3]\n",
    "                \n",
    "            # retrieve gradient and cost\n",
    "            grd = compute_gradient_sigmoid(minibatch_y, minibatch_tx, w) + lambda_ * w\n",
    "            # update step\n",
    "            w -= grd * gamma\n",
    "            \n",
    "            if n_iter > 1: \n",
    "                pl = l\n",
    "            l = (nlog_likelihood(y, tx, w) + lambda_ * w)[0]\n",
    "            if n_iter % 100 == 0: print(f'Iter {n_iter}: {l}')\n",
    "                \n",
    "            # break if the break_threshold (improvements) is met 5 times - idea is that we are no longer really improving    \n",
    "            if n_iter > 1:\n",
    "                if (np.abs(pl-l) < break_threshold):\n",
    "                    break_count += 1\n",
    "            if break_count>=4:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "             \n",
    "    loss = nlog_likelihood(y, tx, w)\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just some weights i found from the unregularized log regression\n",
    "weights_found = np.array([ 0.26142893,  0.86596468, -0.37762987,  0.37808003,  0.38866601,\n",
    "        0.27821863,  0.49783078,  0.27445423,  1.06403986,  0.17753756,\n",
    "        0.2306503 ,  0.02745172,  0.36734549,  0.27717549,  1.15019926,\n",
    "        0.09041755,  0.25066148,  0.03816285,  0.08047934,  0.31177982,\n",
    "        0.29509693,  0.14247221,  0.06900157,  0.03310452,  0.11632592,\n",
    "        0.12584425,  0.1279479 ,  0.27926044,  0.27683506,  0.27699952,\n",
    "        0.27397457,  0.21205993, -0.01097727, -1.04011751,  0.21621489,\n",
    "       -0.40293883,  0.26377053, -0.41471766, -0.42243259,  0.12961204,\n",
    "       -0.02179739, -0.19881119,  0.33088431, -0.40620022, -0.13329768,\n",
    "       -0.15517697,  0.06508222, -0.09240051, -0.17634414,  0.11317156,\n",
    "       -0.10073361, -0.01377299, -0.15074884, -0.26837094,  0.59101823,\n",
    "        0.61882999,  0.6222557 , -0.40088948, -0.40726925, -0.40675704,\n",
    "       -0.1286949 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 50, degree 1\n",
      "Fold 0/10\n",
      "Iter 0: 1158971.6091759978\n",
      "Iter 100: 156697.13534782888\n",
      "Iter 200: 156254.43178298586\n",
      "Iter 300: 155989.08283115827\n",
      "Iter 400: 155843.90492568165\n",
      "Iter 500: 155727.03244342128\n",
      "Iter 600: 155731.089746107\n",
      "Iter 700: 155728.69945278732\n",
      "Iter 800: 155725.24667274658\n",
      "Iter 900: 155725.3799464044\n",
      "Iter 1000: 155726.21716607283\n",
      "Iter 1100: 155721.49583055946\n",
      "Iter 1200: 155720.732461245\n",
      "Iter 1300: 155718.88117060502\n",
      "Iter 1400: 155714.33889566414\n",
      "Fold 1/10\n",
      "Iter 0: 1159111.3475339634\n",
      "Iter 100: 156720.10340406257\n",
      "Iter 200: 156314.27519984602\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-23348dde50d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_best_degree\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_lambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mbest_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_lambdas_per_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_weights_per_degree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-23348dde50d2>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Fold {k}/{k_fold}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreak_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0mloss_te_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mws_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-23348dde50d2>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, gammas, break_threshold)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreak_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mloss_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-49b8903aa12a>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gammas, break_threshold)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# uniform picking of minibatch of a single datapoint in this case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mbreak_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fc10dcee87a6>\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# try training with different lambdas and refine the lambdas depending on what magnitude turns out to be best\n",
    "# same idea with degrees\n",
    "\n",
    "max_iters = 3000\n",
    "gammas = [0.001, 0.0001, .00001, 0.000005]\n",
    "initial_w = weights_found\n",
    "lambdas = np.array([50,100,200])\n",
    "degrees = np.arange(1,10)\n",
    "seed = 12\n",
    "k_fold = 7\n",
    "break_threshold = 1\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree, gammas, break_threshold):\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te, y_tr = y[te_indice], y[tr_indice]\n",
    "    x_te, x_tr = x[te_indice], x[tr_indice]\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    initial_w = np.ones(tx_tr.shape[1])\n",
    "    \n",
    "    w, _ = reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gammas, break_threshold)\n",
    "    \n",
    "    loss_tr = nlog_likelihood(y_tr, tx_tr, w)\n",
    "    loss_te = nlog_likelihood(y_te, tx_te, w)\n",
    "    \n",
    "    return loss_tr, loss_te, w\n",
    "\n",
    "def cross_validation_demo():\n",
    "   \n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "\n",
    "    best_lambdas = []\n",
    "    best_losses = []\n",
    "    best_weights = []\n",
    "\n",
    "    \n",
    "    j = 0 #jth degree\n",
    "    for deg in degrees:\n",
    "        i = 0 #ith lambda\n",
    "        losses_te = []\n",
    "        ws = []\n",
    "        for lambda_ in lambdas:\n",
    "            print(f'Lambda {lambdas[i]}, degree {degrees[j]}')\n",
    "            i += 1\n",
    "            ws_tmp = []\n",
    "            loss_te_tmp = []\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                print(f'Fold {k}/{k_fold}')\n",
    "                loss_tr, loss_te, w = cross_validation(y, x, k_indices, k, lambda_, deg, gammas, break_threshold)\n",
    "                loss_te_tmp.append(loss_te)\n",
    "                ws_tmp.append(w)\n",
    "            losses_te.append(np.mean(loss_te_tmp, axis=0))\n",
    "            ws.append(np.mean(ws_tmp, axis=0))\n",
    "\n",
    "        j += 1\n",
    "        ind_lambda_opt = np.argmin(loss_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_losses.append(losses_te[ind_lambda_opt])\n",
    "        best_weights.append(ws[ind_lambda_opt])\n",
    "        \n",
    "    ind_best_degree = np.argmin(best_losses)\n",
    "\n",
    "    return degrees[ind_best_degree], best_lambdas, best_weights\n",
    "\n",
    "best_degree, best_lambdas_per_degree, best_weights_per_degree = cross_validation_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
